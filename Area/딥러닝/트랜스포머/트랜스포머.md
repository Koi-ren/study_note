- **정의**: 2017년 논문 "Attention is All You Need"에서 나온 모델.
- **핵심 구성**:
    - **셀프 어텐션(Self-Attention)**: 단어 간 관계성 분석. [[셀프 어텐션]] 참고.
    - **멀티헤드 어텐션**: 여러 관점에서 관계 파악.
    - **포지셔널 인코딩**: 단어 순서 정보 추가.
- **장점**:
    - 긴 시퀀스 처리 가능.
    - 병렬 연산으로 속도 빠름.
- **영향**: [[BERT]], [[GPT]] 같은 모델의 기반 됨. [[텍스트 마이닝]]에도 활용.