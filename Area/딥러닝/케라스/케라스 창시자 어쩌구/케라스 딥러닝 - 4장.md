---
작성 시작 연월일: 2025-03-26
작성 종료 연월일: 0001-01-01
실습환경: Google - colab
사용 툴:
  - jupyter notebook
  - keras
교재: DEEP LEARNING with Python 2/e, 프랑소와 숄레
교재 진도: 4장
tags:
  - 딥러닝
  - 수학
  - 알고리즘
  - python
  - 모델링
  - 데이터
  - 신경망
  - 케라스
---
[[분류와 회귀에서 사용하는 용어]]

# 4.1 영화 리뷰 분류 : 이진 분류 문제
#영화리뷰문제 #이진분류문제

2종 분류(two-class classification) 또는 이진 분류(binary classification)는 아마도 가장 널리 적용된 머신 러닝 문제임

이번 예제에서 리뷰 텍스트 기반으로 영화 리뷰를 긍정(positive)과 부정(negative)으로 분류하는 법을 배우겠다.

### 4.1.1 IMDB 데이터셋

Internet Movie Database(IMDB) : 영화 양극단 리뷰 5만개 데이터셋

데이터셋 구성:
- 훈련 데이터 : 25,000개
- 테스트 데이터 25,000개
- 긍정 부정 비율 - 50 : 50

데이터 설명 :
- 데이터 전처리 완료
- 리뷰(단어 시퀸스)가 숫자 시퀸스로 변환
- 각 숫자는 사전[^1]에 있는 고유한 단어를 나타냄

```python
#IMDB 데이터셋 로드
from tensorflow.keras.datasets import imdb

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
```

- `num_word=10000` 매개 변수는 훈련데이터에서 가장 자주 나타나는 단어 1만 개만 사용하겠다는 의미
	- 10000개 보다 드물게 나타나는 단어는 무시
	- 위 매개 변수 덕분에 적절한 크기의 벡터 데이터를 얻을 수 있음
		- 위 매개 변수 미 적용 시 훈련 데이터에 85,585개의 고유 단어가 포함되기 때문
	- 너무 많은 데이터는 불필요하며 많은 데이터가 하나의 샘플에 등장
		- 분류 작업에 의미 있게 사용 가능


- 변수 `train_data`와 `test_data`는 리뷰를 담은 배열임
	- 각 리뷰는 단어 인덱스의 리스트
	- 단어 시퀸스가 인코딩된 것 - [[딥러닝 강의 요약#임베딩(Embedding)]]
- 변수 `train_labels`와 `test_labels`는 부정(0)과 긍정(1)을 나타내는 1의 리스트

```python
>>> print(train_data[0])

[1, 14, 22, 16,. . . 178, 32]
---
>>> print(train_labels[0])

1
```

가장 자주 등장하는 단어를 1만 개로 제한했기에[^2] 단어 인덱스는 9,999를 넘지 않음

```python
>>> max([max(sequence) for sequence in train_data])

9999
```

리뷰 데이터 하나를 원래 영어로 바꾸는 방법

```python
#리뷰를 다시 텍스트로 디코딩하기

# word_index는 단어와 정수 인덱스를 매핑한 딕셔너리임
word_index = imdb.get_word_index()

# 정수 인덱스와 단어를 매핑하도록 뒤집기
reverse_word_index = dict(

    [(value, key) for (key, value) in word_index.items()])

# 리뷰를 디코딩
# 0, 1, 2는 'padding','문서 시작', '사전에 없음' 을 위해 예약되어 있기에 인덱스에서 3을 뺌
decoded_review = " ".join(

    [reverse_word_index.get(i -3, "?") for i in train_data[0]])

print(decoded_review)

---실행결과---
? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all
```

### 4.1.2 데이터 준비

- 신경망에 숫자 리스트를 바로 주입할 수 없음
	- 신경망은 동일한 크기의 배치를 기대하기 때문

리스트를 텐서로 바꾸는 방법
- 첫 번째 방법(벡터화, vectorization)
	- 같은 길이가 되도록 리스트에 `padding`을 추가
	- `(samples, max_length)` 크기의 정수 텐서로 변환
		- 가장 긴 리뷰는 2,494개의 단어로 구성, 이를 훈련 데이터로 변환 시 텐서의 크기 = (25000, 2494)
	- 정수 텐서를 다룰 수 있는 층으로 신경망을 시작 
		- [[워드 임베딩(Word Embedding)]] 참조
- 두 번째 방법(멀티-핫 인코딩)
	- 리스트를 [[멀티-핫 인코딩(multi-hot encoding)]]  

본 예제에서는 두 번째 방식을 차용하고, 데이터를 멀티-핫 벡터로 만들겠음


```python
# 정수 시퀸스를 멀티-핫 인코딩으로 인코딩하기

import numpy as np

def vectorize_sequences(sequences, dimension=10000):
  # 크기가 (len(sequences), dimension)이고 모든 원소가 0인 행렬을 만듬
  results = np.zeros((len(sequences), dimension))

  for i, sequence in enumerate(sequences):

    for j in sequence:
	  # results[i]에서 특정 인덱스의 위치를 1로 만듬
      results[i, j] = 1.

  return results
  
x_train = vectorize_sequences(train_data) # 훈련 데이터를 벡터로 변환
x_test = vectorize_sequences(test_data) # 테스트 데이터를 벡터로 변환
```

위 코드를 통해 벡터 변환된 데이터를 다음과 같이 살펴볼 수 있다

```python
>>> x_train[0]

array([0., 1., 1., ..., 0., 0., 0.])
```

또한 레이블은 다음과 같이 쉽게 벡터로 바꿀 수 있다

```python
y_train = np.asarray(train_labels).astype("float32")
y_test = np.asarray(test_labels).astype("float32")
```

### 4.13 신경망 모델 만들기

입력 데이터가 벡터, 레이블은 스칼라일 때 가장 잘 작동하는 모델은?
$\rightarrow$ `relu` 활성화 함수를 사용한 밀집 연결 층을 그냥 쌓는 것

`Dense`층을 쌓을 때 두 가지 중요한 구조 상의 결정 요소
- 얼마나 많은 층을 사용할 것인가?
- 각 층에 얼마나 많은 유닛을 둘 것인가?



















[^1]: 데이터셋의 전체 문서에 나타난 모든 단어에 고유한 번호를 부여한 목록을 어휘 사전 또는 사전이라고 부르고 이런 작업은 텍스트 데이터를 다룰 때 기본적으로 수행하는 전처리 과정임
	
	참고자료 : 
	[[워드 임베딩(Word Embedding)]]
	[[텍스트 데이터 전처리]]

[^2]: `num_words=10000`, 본 노트의 47, 50 번 줄에서 설명했다
