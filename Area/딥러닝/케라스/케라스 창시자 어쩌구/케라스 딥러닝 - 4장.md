---
작성 시작 연월일: 2025-03-26
작성 종료 연월일: 0001-01-01
실습환경: Google - colab
사용 툴:
  - jupyter notebook
  - keras
교재: DEEP LEARNING with Python 2/e, 프랑소와 숄레
교재 진도: 4장
tags:
  - 딥러닝
  - 수학
  - 알고리즘
  - python
  - 모델링
  - 데이터
  - 신경망
  - 케라스
---
[[분류와 회귀에서 사용하는 용어]]

# 4.1 영화 리뷰 분류 : 이진 분류 문제
#영화리뷰문제 #이진분류문제

2종 분류(two-class classification) 또는 이진 분류(binary classification)는 아마도 가장 널리 적용된 머신 러닝 문제임

이번 예제에서 리뷰 텍스트 기반으로 영화 리뷰를 긍정(positive)과 부정(negative)으로 분류하는 법을 배우겠다.

### 4.1.1 IMDB 데이터셋

Internet Movie Database(IMDB) : 영화 양극단 리뷰 5만개 데이터셋

데이터셋 구성:
- 훈련 데이터 : 25,000개
- 테스트 데이터 25,000개
- 긍정 부정 비율 - 50 : 50

데이터 설명 :
- 데이터 전처리 완료
- 리뷰(단어 시퀸스)가 숫자 시퀸스로 변환
- 각 숫자는 사전[^1]에 있는 고유한 단어를 나타냄

```python
#IMDB 데이터셋 로드
from tensorflow.keras.datasets import imdb

(train_data, train_labels), (test_data, test_lable) = imdb.load_data(num_words=10000)
```

- `num_word=10000` 매개 변수는 훈련데이터에서 가장 자주 나타나는 단어 1만 개만 사용하겠다는 의미
	- 10000개 보다 드물게 나타나는 단어는 무시
	- 위 매개 변수 덕분에 적절한 크기의 벡터 데이터를 얻을 수 있음
		- 위 매개 변수 미 적용 시 훈련 데이터에 85,585개의 고유 단어가 포함되기 때문
	- 너무 많은 데이터는 불필요하며 많은 데이터가 하나의 샘플에 등장
		- 분류 작업에 의미 있게 사용 가능


- 변수 `train_data`와 `test_data`는 리뷰를 담은 배열임
	- 각 리뷰는 단어 인덱스의 리스트
	- 단어 시퀸스가 인코딩된 것 - [[딥러닝 강의 요약#임베딩(Embedding)]]
- 변수 `train_labels`와 `test_labels`는 부정(0)과 긍정(1)을 나타내는 1의 리스트

```python
>>> print(train_data[0])

[1, 14, 22, 16,. . . 178, 32]
---
>>> print(train_labels[0])

1
```

가장 자주 등장하는 단어를 1만 개로 제한했기에[^2] 단어 인덱스는 9,999를 넘지 않음

```python
>>> max([max(sequence) for sequence in train_data])

9999
```

리뷰 데이터 하나를 원래 영어로 바꾸는 방법

```python
word_index = imdb.get_word_index()

reverse_word_index = dict(

    [(value, key) for (key, value) in word_index.items()])

decoded_review = " ".join(

    [reverse_word_index.get(i -2, "?") for i in train_data[0]])

print(decoded_review)

---실행결과---
? that on as about parts admit ready speaking really care boot see holy and again who each a are any about brought life what power ? br they sound everything a though and part life look ? fan recommend like and part elegant successful for feeling from this based and take what as of those core movie that on and manage airplane 4 and on me because i as about parts from been was this military and on for kill for i as cinematography with ? a which let i is left is two a and seat raises as sound see worried by and still i as from running a are off good who scene some are church by of on i come he bad more a that gives as into ? is and films best commenting was each and ? to rid a beyond who me about parts final his keep special has to and ? manages this characters how and perhaps was american too at references no his something of enough russ with and bit on film say final his sound a back one jews with good who he there's made are characters and bit really as from harry how i as actor a as transfer plot think at was as inexplicably movie quite at
```

### 4.1.2 데이터 준비

- 신경망에 숫자 리스트를 바로 주입할 수 없음
	- 신경망은 동일한 크기의 배치를 기대하기 때문

리스트를 텐서로 바꾸는 방법
- 첫 번째 방법(벡터화, vectorization)
	- 같은 길이가 되도록 리스트에 `padding`을 추가
	- `(samples, max_length)` 크기의 정수 텐서로 변환
		- 가장 긴 리뷰는 2,494개의 단어로 구성, 이를 훈련 데이터로 변환 시 텐서의 크기 = (25000, 2494)
	- 정수 텐서를 다룰 수 있는 층으로 신경망을 시작 
		- [[워드 임베딩(Word Embedding)]] 참조
- 두 번째 방법(멀티-핫 인코딩)
	- 리스트를 [[멀티-핫 인코딩(multi-hot encoding)]]  

























[^1]: 데이터셋의 전체 문서에 나타난 모든 단어에 고유한 번호를 부여한 목록을 어휘 사전 또는 사전이라고 부르고 이런 작업은 텍스트 데이터를 다룰 때 기본적으로 수행하는 전처리 과정임
	
	참고자료 : 
	[[워드 임베딩(Word Embedding)]]
	[[텍스트 데이터 전처리]]

[^2]: `num_words=10000`, 본 노트의 47, 50 번 줄에서 설명했다
