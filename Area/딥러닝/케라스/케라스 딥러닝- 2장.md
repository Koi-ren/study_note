---
작성 시작 연월일: 2025.02.26
작성 종료 연월일: 2025.03.06
실습환경: Google - colab
사용 툴:
  - jupyter notebook
  - keras
교재: DEEP LEARNING with Python 2/e, 프랑소와 숄레
교재 진도: 2장
tags:
  - 텐서
  - 딥러닝
  - 자료구조
  - 텐서연산
  - 신경망
  - 케라스
  - 프로젝트
  - 모델링
  - 데이터
  - 연쇄법칙
  - python
---
# 기본 수학 개념[^1]
---
# 2.1 신경망과의 첫 만남

가장 역사적인 문제 데이터 셋인 MNIST를 사용
MNIST는 Numpy 배열 형태로 keras에 이미 포함됨

 ```Python
 from tensorflow.keras.datasets import mnist
 (train_images, train_labels),(test_images, test_labels) = mnist.load_data()
 ```

- train_OOO == 훈련 세트(traing set)

- test_OOO == 테스트 세트(test set)

```Python
>>> train_images.shape


(60000, 28, 28)
---
>>> len(train_labels)


60000
---
>>> train_labels


array([5, 0, 4 ..., 5, 6, 8], dtype=uint8)
```

테스트 세트 역시 똑같다.


> [!기억하자]
> > 통계 분석이나 머신 러닝에서 그렇듯, 어떠한 분석을 하기 전,
>  데이터를 보고 "어떻게 모델링 할 것인지", "왜 해야 하는지", 
>  "이것이 왜 필요한지"를 제대로 정의해야 한다.

**작업 순서는 다음과 같다**
 1. train_images와 train_labels를 네트워크에 주입
 2. 네트워크에서 이미지와 레이블을 연관 학습
 3. test_images에 대한 예측을 네트워크에 요청
 4. 예측과 실제 test_labels이 맞는지 확인

```Python
## 신경망 구조

from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
	layers.Dense(512, activation="relu"),
	layers.Dense(10, activation="softmax") #마지막 층
])
```

- 신경망의 핵심 구성 요소는 **층(layers)** 이다.
	 - 층은 데이터를 위한 **필터(filter)** 역할
	 - 주어진 문제에 더 의미있는 **표현(represntation)** 을 입력된 데이터로부터 추출
	 - 대부분의 딥러닝은 간단한 층을 연결하여 구성
	 - 층들이 점진적으로 데이터를 정제하는 형태로 구성됨

위 코드는 **완전 연결(fully connected)** 된 신경망 층인 Dense 층 2개 연속으로 구성됨.
마지막 층은 10개의 확률 점수 배열을 반환하는 **softmax** 분류층임.

신경망이 훈련준비를 마치기 위해 컴파일 단계에 포함되야하는 삼원소
- **optimizier** : 
  `성능을 향상을 위해 입력 데이터 기반으로 모델을 업데이트하는 메커니즘`
- **loss function** : 
  `훈련 데이터에서모델 성능을 측정, 모델의 학습 보조 (과적합(overfitting) 방지)`
- **훈련과 테스트 과정을 모니터링할 지표** : 
  `이번 모델에서는 정확도 만을 채택함`

```Python
## 컴파일 단계
model.compile(optimizer="rmsprop",
              loss="sparse_categorical_crossentropy",
              metrics=["accuracy"])
```

훈련 시작 전 데이터 전처리
- 데이터 내의 모든 값을 0과 1의 스케일로 조정 (정규화 작업 등)
- 훈련 이미지 == [0, 255] 사이의 'uint8' type에 (60000, 28, 28) 크기의 배열
- 정규화 데이터 == [0, 1] 사이의 'float32' type에 (60000, 28 * 28) 크기의 배열

```Python
## 데이터 전처리

train_images = train_images.reshape(60000, 28*28)
train_images = train_images.astype("float32") /255
test_images = test_images.reshape(10000, 28*28)
test_images = test_images.astype("float32") /255
```

이제 모델 훈련을 시작 --> **fit()** 함수 사용

```Python
#모델 훈련

>>> model.fit(train_images, train_labels, epochs=5, batch_size=128)

Epoch 1/5
469/469 ━━━━━━━━━━━━━━━━━━━━ 5s 8ms/step - accuracy: 0.8744 - loss: 0.4331
Epoch 2/5
469/469 ━━━━━━━━━━━━━━━━━━━━ 4s 8ms/step - accuracy: 0.9661 - loss: 0.1168
Epoch 3/5
469/469 ━━━━━━━━━━━━━━━━━━━━ 4s 8ms/step - accuracy: 0.9782 - loss: 0.0722
Epoch 4/5
469/469 ━━━━━━━━━━━━━━━━━━━━ 4s 8ms/step - accuracy: 0.9845 - loss: 0.0510
Epoch 5/5
469/469 ━━━━━━━━━━━━━━━━━━━━ 5s 7ms/step - accuracy: 0.9901 - loss: 0.0360
```

acurracy와 loss function을 확인할 수 있음.
이제 훈련 모델을 통해 새로운 숫자 이미지에 대한 클래스 확률을 예측 할 수 있음.

위 훈련 결과를 확인 시 2번쨰 Epoch(훈련)에서 부터 96%의 정확도를 달성하고 있으며 0.990까지 금방 달성하는 모습을 볼 수 있음.

```Python
## 모델 사용해 예측 만들기

>>> test_digits = test_images[0:10]
>>> predictions = model.predict(test_digits)
>>> predictions[0]

1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step

array([4.7389855e-07, 1.8744069e-08, 8.0541788e-05, 1.1161435e-03,
       1.3907619e-11, 1.8572474e-07, 1.5570803e-11, 9.9879193e-01,
       1.6225046e-06, 9.0445392e-06], dtype=float32)
```

__출력된 배열의 인덱스 i 숫자__ == __숫자 이미지 test_digits[0] 이 클래스 i에 속할 확률__

첫번째 테스트 숫자는 인덱스 7에서 9.9879e-01(약 99.87%)에 달하는 높은 확률 값을 얻었고, 따라서 이 사진의 경우 모델의 예측 결과는 7이 됨.

```Python
>>> predictions[0].argmax()

7
---
>>> predictions[0][7]

0.99879193
---
>>>test_labels[0]

7
```

<br>
모델이 평균적으로 학습한 적 없는 test 데이터에 대한 전체 평균 정확도를 확인해보자.

```python
>> test_loss, test_acc = model.evaluate(test_images, test_labels)

313/313 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.9099 - loss: 0.3189
---
>>> print(f"테스트 정확도: {test_acc}")

테스트 정확도: 0.9217000007629395
```

테스트 세트의 정확도는 92%로 훈련 세트 99% 보다는 약간 낮음

훈련 정확도와 테스트 정확도의 차이는 **과대적합(overfitting)** 때문임.



> [!NOTE] 과대적합이란?
> 머신 러닝 모델이 훈련 데이터보다 새로운 데이터에서 성능이 낮아지는 경향


---
# 2.2 - 신경망을 위한 데이터 표현


모든 머신 러닝 시스템은 일반적으로 **텐서(tensor)** 를 기본 데이터 구조로 사용

> [!NOTE] Tensor란?
> 간단하게 설명하면, 데이터를 위한 컨테이너(container), 일반적으로 수치형 데이터를 다루므로 숫자를 위한 컨테이너다.

텐서는 임의의 차원 개수를 가지는 행렬의 일반화된 모습
- 텐서에서는 **차원(demension)** 을 **축(axis)** 이라고 종종 부른다 

## 스칼라(rank-0 tensor)
1. 하나의 숫자만을 담고 있는 텐서를 **스칼라(scalar)** 라고 부름.
2. Numpy에서 float32, float64 type의 숫자는 스칼라 텐서(또는 배열 스칼라(array scalar))라고 부름
3. 스칼라 텐서의 축 개수는 0이며 이 텐서의 축 개수를 랭크(rank)라고 함.

```python
>>> import numpy as np
>>> x = np.array(12)
>>> x

array(12)
---
>>> x.ndim

0
```

## 벡터(rank-1 tensor)
1. 숫자의 배열을 **벡터(vector)** 또는 랭크 1텐서, 1D 텐서라고 부름
2. 랭크-1 텐서는 하나의 축을 가짐.
```python
>>> x = np.array([12, 3, 4, 14, 7])
>>> x

array([12, 3, 4, 14, 7])
---
>>> x.ndim

1
```


이 벡터의 경우 5개의 원소를 가지고 있음 ==> 5차원 벡터

### **주의 : 5D 벡터 != 5D 텐서**
- 5D 벡터는 하나의 축을 따라 5개의 차원을 가진 것
- 5D 텐서는 5개의 축을 가진 것(텐서의 각 축을 따라 여러 개의 차원을 가진 벡터가 놓을 수 있음)

**차원 수(dimensionality)** 는 특정 축을 따라 놓은 원소 개수(5D 벡터와 같은 경우)이거나 텐서의 축 개수를 의미하므로 혼동 가능성 있음

후자의 경우 랭크가 5인 텐서라고 말하는 것이 기술적으로 정확함.
~~그냥 좋은 말 할때 랭크-i 텐서라고 불러라~~

## 행렬(rank-2 tensor)
1. 벡터의 배열은 **행렬(matrix)** 또는 랭크-2 텐서나 2D 텐서이다.
2. 행렬에는 2개의 축이 있다.(이를 우리는 행(row)과 열(column)이라고 부름)

행렬은 숫자가 채워진 사각 격자라고 생각할 수 있음

```python
>>> x = np.array([[5, 78, 2, 34, 0],
				   6, 79, 3, 35, 1],
				   [7, 80, 4, 36,2]])
>>> x.ndim

2
```

- 첫 번째 축에 놓여 있는 원소를 **행** 
- 두 번째 축에 놓여 있는 원소를 **열** 
- 위 코드의 행렬의 경우 --> 행 = [5, 78, 2, 34, 0], 열 = [5, 6, 7]

## rank-3 tensor와 더 높은 랭크의 텐서
1. 여러 행렬을 하나의 새로운 배열로 합치면 숫자가 채워진 직육면체 형태로 해석
2. 이것을 랭크-3 텐서(3D 텐서)라고 부름.

```python
>>> x = np.array([[[5, 78, 2, 34, 0],

               [6, 79, 3, 35, 1],

               [7, 80, 4, 36, 2]],

              [[5, 78, 2, 34, 0],

               [6, 79, 3, 35, 1],

               [7, 80, 4, 36, 2]],

              [[5, 78, 2, 34, 0],

               [6, 79, 3, 35, 1],

               [7, 80, 4, 36, 2]]])

>>> x.ndim

3
```

여러 개의 랭크-3 텐서를 하나의 배열로 합치면 그것이 랭크-4 텐서를 만드는 식임

딥러닝에서는 보통 랭크-0 ~ 랭크-4 텐서까지 다루고 동영상 데이터를 다룰 경우 랭크-5 텐서까지도 감.

## 핵심 속성
텐서는 3개의 핵심 속성으로 정의됨.

**1. 축의 개수(랭크) :**
	- 랭크-3 텐서에는 3개의 축, 행렬에는 2개의 축이 존재.

** 2. 크기(shape) : **
	- 텐서의 각 축을 따라 얼마나 많은 차원이 있는 지를 나타낸 파이썬의 튜플(tuple).
	- 앞서 나온 행렬의 경우 크기는 (3, 5), 랭크-3 텐서의 크기는 (3, 3, 5) 이다.

**3. 데이터 타입(data type, 이하 dtype) :**
	- 텐서에 포함된 데이터의 타입 
	- float16, float32, float64, uint8, string 등이 포함됨

아까 봤던 MNSIT의 핵심 속성을 알아보자

```python
### 축의 개수
>>> train_images.ndim

3
---
### 배열의 크기(shape)
>>> train_images.shape

(60000, 28, 28)
---
### 데이터 타입
>>> train_images.dtype

uint8
```

- 8비트 정수형 랭크-3 텐서
- $28 \times 28$ 크기의 정수 행렬 6만 개

### 핵심 속성 부록
MNIST 의 5번째 이미지를 출력하는 방법은 다음과 같다.

```python
import matplotlib.pyplot as plt

digit = train_images[4]

plt.imshow(digit, cmap=plt.cm.binary)

plt.show()
```
![[Pasted image 20250228232330.png|500]]


## 넘파이로 텐서 조직하기

위에서 우리는 train_images[4]를 통해 이미지를 선택, 출력함.
배열에 있는 특정 원소를 선택하는 것을 **슬라이싱(slicing)** 이라고 함.

다음 예는 11번째에서 101번째까지 숫자를 선택하여 (90, 28, 28) 크기의 배열을 만드는 것임

```python
>>> my_slice = train_images[10:100]
>>> my_slice.shape

(90, 28, 28)
```

조금 더 자세한 표기법은 각 배열의 축을 따라 슬라이싱의 시작 인덱스와 마지막 인덱스를 지정하는 것 [ : (콜론)은 전체 인덱스를 선택하는 것임]

```python
>>> my_slice = trian_images[10:100, :, :]
>>> my_slice.shape

(90, 28, 28)
---
>>> my_slice = train_images[10:100, 0:28, 0:28]
>>> my_slice.shape

(90, 28, 28)
```

각 배열의 축을 따라 어떤 인덱스 사이도 선택 가능함.
이미지의 오른쪽 아래 $14 \times 14$ 픽셀을 선택하는 코드는 아래와 같음
```python
my_slice = train_images[:, 14:, 14:]
```

- 첫 번째 ":" 는 전체 이미지 범위 (모든 이미지를 포함)를 의미
- 첫 번째 "14:"는 14번째 인덱스부터 끝까지를 뜻하며, 이미지의 위에서 아래를 의미
- 두 번째 "14:"도 마찬가지며 왼쪽에서 오른쪽으로 슬라이싱

따라서, 원래 이미지의 오른쪽과 아래쪽 부분을 선택하게 되며, 슬라이싱 결과는 원본 이미지의 왼쪽 위 부분을 제외한 나머지 부분이 됨.

여기서는 데이터의 크기(shape) $28 \times 28$ 크기의 정수 행렬 6만 개 이기에 위 코드를 통해 슬라이싱 된 이미지는 오른쪽 아래 $14 \times 14$ 가 된다.

음수 인덱스도 사용 가능하며 현재 축의 끝에서 상대적 위치를 나타냄.

정중앙에 위치한 $14 \times 14$ 픽셀 조각을 슬라이싱 하는 코드는 아래와 같음
```python
my_slice = train_images[:, 7:-7, 7:-7]
```
## 배치 데이터
- 일반적으로 딥러닝에서 사용하는 모든 데이터 텐서의 첫 번째 축(axis)은 **샘플 축(sample axis)** 이다. 
- 이는 인덱스가 0부터 시작하기 때문이다.
- MNIST 예제에서는 숫자 이미지가 샘플임임

딥러닝 모델은 한번에 전체 데이터 셋을 처리하지 않음.
전체 데이터 셋을 **배치(batch)** 로 나눠서 처리함

이는 이전에 언급한 슬라이싱과 매우 깊은 연관이 있으며 MNIST 숫자 데이터에서 크기 128 배치 하나는 아래와 같음.
```python
batch = train_images[:128]
```
그 다음 배치는 당연히 아래와 같다
```python
batch = train_images[128:256]
```
위 규칙을 아래와 같이 표현이 가능하다.
```python
batch = train_images[128*n:128*(n+1)]
```
이런 배치 데이터를 다룰 때 첫 번째 축(0번 축)을 **배치 축(batch axis)** or **배치 차원(batch dimension)** 이라고 함.
> [!NOTE] 용어에 대하여
> 실무에 투입 시 위에서 설명하는 용어에 대해서 반드시 알아야 커뮤니케이션에 지장이 없을 것이다. <br><br>반드시 숙지해서 본인 것으로 만들고, 면접 시 버벅이지 않는 것을 목표로 하자!

## 데이터의 종류

1. **벡터 데이터**
	- 대부분의 경우 해당
	- 하나의 데이터 포인트가 벡터로 인코딩 가능성 있음
	- 배치 데이터는 랭크-2 텐서로 인코딩
	- 일반적으로 첫 번째 축은 **샘플 축**, 두 번째 축은 **특성 축(feature axis)** 
	- ex : 인구 통계 데이터(나이, 성별, 소득) -> 사람은 3개 값 벡터, 10만 명이 포함된 데이터 셋은 (100000, 3), 랭크-2 텐서로 저장
<br>
1. **시계열 데이터 && 시퀀스 데이터**
	- 시간 혹은 연속된 순서가 중요할 경우 이 정보가 포함되어 랭크-3텐서로 저장.
	- 각 샘플은 벡터 신퀀스로 인코딩 되며, 랭크-3 텐서로 인코딩.
	- ![[Pasted image 20250301002938.png|500]]
	- 시간 축은 항상 두 번째 축(index == 1)
	- ex : 주식 가격 데이터, 트위터 데이터 셋
<br>
3. **이미지 데이터**
	- 이미지는 높이, 너비 컬러의 3차원으로 구성
	- ![[Pasted image 20250301003433.png|500]]
	- 이미지 텐서의 크기는 채널 마지막 방식과 채널 우선 방식
	- 채널 마지막 방식(channel-last) : 컬러 채널의 깊이를 끝에 놓는 것```(sample, height, width, color_depth)```
	- 채널 우선 방식(channel-first) : 채널의 깊이를 배치 축 바로 뒤에 놓는 것```(samples, color_depth, height, width)```
<br>
4. 동영상 데이터
   - 랭크-5 텐서가 필요한 데이터
   - 영상은 사진(프레임)의 연속이고 여러 비디오의 배치는 ```(samples, frames, height, width, color_depth)``` 로 랭크-5 텐서로 저장될 수 있음

---
# 2.3 신경망의 톱니바퀴 : 텐서 연산

- 심층 신경망이 학습한 모든 변환을 수치 데이터 텐서에 적용하는 몇 종류의 **텐서연산(tensor operation)(또는 텐서함수(tensorfunction))** 이라 함.

```python
keras.layers.Dense(512, activation="relu)
```
위 코드는 행렬 입력을 받고 입력 텐서의 새로운 표현인 또 다른 행렬을 반환하는 함수

이를 구체적으로 보면 

$output = relu(dot(W, input)+b)$

W(가중치) == 행렬, b(편향) == 벡터... 둘 모두 층의 속성

## 원소별 연산

relu 함수와 덧셈은 원소별 연산(element-wise operation)이다.
이는 텐서 각 원소에 독립적으로 적용 --> **병렬 구현 가능**

```python
def naive_relu(x):
	assert len(x.shape) == 2 #x는 rank-2 tensor, numpy array이다.
	x = x.copy()
	for i in range(x.shape[0]:
		for j in range(x.shape[1]):
			x[i, j] = max(x[i, j], 0)
	return x
```

덧셈도 동일함

```python
def nave_add(x, y):
	assert(x.shape) == 2
	assert x.shape == y.shape
	x = x.copy()
	for i in range(x.shape[0])
		for j in range(x.shape[1])
			x[i, j] += y[i, j]
	return x
```

위와 같은 원리로 뺄셈과 곱셈 등을 할 수 있음
고도로 병렬화 시켜 완전히 벡터와된 CUDA 구현을 통해 원소별(element-wise) 연산이 실행됨

## 브로드캐스팅(broadcasting)

- naive_add 는 동일한 크기의 rank-2 tensor만 지원함
- 하지만 Dense 층에서는 rank-2 tensor와 vector를 더함

계산이 모호하지 않고 실행 가능하다면 
--> **작은 텐서가 큰 텐서의 크기에 맞춰 브로드캐스팅(broadcasting)됨**

**브로드캐스팅 단계**
1. 큰 텐서의 ndim에 맞도록 작은 텐서에 축(broadcasting axis)이 추가됨
2. 작은 텐서가 새 축을 따라서 큰 텐서의 크기에 맞도록 반복됨

```python
import numpy as np

x = np.random.random((32, 10)) # x는 (32, 10)의 크기를 가진 랜텀한 행렬
y = np.random.random((10,)) # y는 (10,)의 크기를 가진 랜덤한 벡터

y = np.expand_dims(y, axis=0) # y의 크기를 (1, 10)로 바꿈

Y = np.concatenate = ([y]*32, axis=0) # 축 0을 따라 y를 32번 반복해 크기가 (32, 10)인 Y를 얻음
```

이제 X랑 Y의 크기가 같아 더할 수 있음.

구현 입장에서는 새로운 텐서가 만들어지면 매우 비효율적이어서 어떤 rank-2 tensor도 만들어지지 않고, 위에서 반복된 연산은 완전히 가상적임

이를 단순하게 구현한다면...

```python
def naive_add_matrix_and_vector(x, y):
	assert len(x.shape) == 2 # x는 rank-2 tensor, numpy array임
	assert len(y.shape) == 1 # y는 numpy vector

	assert x.shape[1] == y.shape[0]
	x = x.copy()
	for i in range(x.shape[0]):
		for j in range(x.shape[1]):
			x[i, j] += y[j]

	return x
```

## 텐서 곱셈

- 텐서 곱셈(tensor product) 또는 점곱(dot product)은 가장 널리 사용되는 텐서 연산임.
- 넘파이에서 텐서 곱셈은 np.dot함수를 사용하여 수행

```python
x = np.random.random((32,))
y = np.random.random((32,))
z = np.dot(x, y)
```

이를 수학에서는 점곱으로 표현

$z = x \cdot y$  

```python
def naive_vector_dot(x, y):
	assert len(x.shape) == 1
	assert len(y.shape) == 1
	assert x.shape[0] == y.shape[0]
	z = 0.
	for i in range(x.shape[0]):
		z += x[i] * y[i]
	
	return z
```

위 코드에서 보이듯 **두 벡터의 점곱은 스칼라가 됨**
$\Rightarrow$ 원소 개수가 같은 벡터끼리 점곱이 가능함

행렬 x와 벡터 y 사이에도 점곱이 가능함
$\Rightarrow$ **y와 x의 행 사이에서 점곱이 일어나 벡터가 반환됨**
.
.
.
입력과 출력을 배치해보면 어떤 크기의 점곱이 가능한지 알 수 있음

![[Pasted image 20250304225307.png|500]]

x의 행 벡터와 y의 열 벡터가 같은 크기여야 하므로 자동으로 x의 너비는 y의 높이와 동일해지고, 2D의 경우처럼 크기를 맞추는 동일한 규칙을 따르면 다음과 같이 고차원 텐서 간의 점곱을 할 수 있음.

$(a, b, c, d) \cdot (d,) \rightarrow (a, b, c)$ ..........................(1)
$(a, b, c, d) \cdot (d, e) \rightarrow (a, b, c, e)$ ..........................(2)

1. $(a, b, c, d) \cdot (d,) \rightarrow (a, b, c)$ ..........................(1):
    
    - 첫 번째 텐서 $(a, b, c, d)$는 4차원 배열임
        
    - 두 번째 텐서 $(d,)$는 1차원 배열임
        
    - 여기서 $d$ 차원이 동일하므로, 첫 번째 텐서의 $d$ 요소와 두 번째 텐서의 $d$ 요소가 곱해지며, 그 결과 $(a, b, c)$의 3차원 배열이 됨
        
2. $(a, b, c, d) \cdot (d, e) \rightarrow (a, b, c, e)$ ..........................(2):
    
    - 첫 번째 텐서 $(a, b, c, d)$는 4차원 배열임
        
    - 두 번째 텐서 $(d, e)$는 2차원 배열임
        
    - 여기서 $d$ 차원이 동일하므로, 첫 번째 텐서의 $d$ 요소와 두 번째 텐서의 $d$ 요소가 곱해지며 그 결과 $(a, b, c, e)$의 4차원 배열이 됨
## 텐서 크기 변환

지금까지 꼭 알아봐야할 텐서 연산에 대해 알아봤다.
1. 원소별 연산(element-wise operation) : relu, add 등
2. 브로드캐스팅(brodcasting) - 크기가 다른 텐서 사이의 계산
3. 텐서 곱셈(tensor product) - 행렬 점곱 등

꼭 알아둬야 할 다음 텐서 연산은 **텐서 크기 변환(tensor reshaping)**이다.

첫번째 신경망 예제에서 모델에 주입할 숫자 데이터를 전처리할 떄 사용했음.

```python
train_images = train_images.reshape((6000,28*28))
```

```python
>>>import numpy as np
>>>x = np.array([[0., 1.],
              [2., 3.],
              [4., 5.]])
>>>x.shape

(3, 2)
---
>>>x = x.reshape((6, 1))
>>>x.shape

(6, 1)
---
>>> x

array([[0.],
       [1.],
       [2.],
       [3.],
       [4.],
       [5.]])
---
>>>x.reshape((2,3))
>>>x.shape

(2, 3)
>>>x

array([[0., 1., 2.],
       [3., 4., 5.]])
---
>>>x = np.zeros((300, 20))
>>>x = np.transpose(x) # 전치행렬
>>>x.shape

(20, 300)
```

[[빠른 텐서 연산을 위한 자료구조]]

### 파이썬 리스트가 비효율적인 이유

1. 파이썬에서는 숫자가 객체로 처리되어 메모리 효율성이 낮음.
    
2. 파이썬 리스트는 객체들의 포인터 집합으로 최적화되어 있지 않음.
    
3. 파이썬의 느린 성능으로 대규모 수학 연산에 부적합함.
    

### 효율적인 텐서 다루기

- **메모리 할당 방식**: PyTorch와 NumPy의 텐서는 메모리 상에서 인접한 곳에 저장됨.
    
- **메모리 절약**: 다양한 자료형을 제공하여 메모리 사용을 최적화함.
    
- **인덱싱과 뷰**: 텐서의 일부를 인덱싱할 때 새로운 메모리를 할당하지 않고, 원본 텐서를 다른 관점으로 바라봄
    
- **자료구조**: 텐서의 모든 원소는 1차원 배열 형태의 storage에 저장되며, offset과 stride를 통해 접근함.


## 텐서연산의 기하학적 해석

- 텐서의 내용은 어떤 기하학적 공간에 있는 좌표 포인트로 해석 가능
- 따라서 모든 텐서 연산은 기하학적 해석이 가능함

1. 이동
2. 회전
3. 크기 변경
4. 선형 변환
5. 아핀 변환
6. relu 활성화 함수를 사용하는 Dense 층

![[Pasted image 20250305231254.png|500]]

## 딥러닝의 기하학적 해석

신경망은 전체적으로 텐서 연산의 연결로 구성

모든 텐서 연산은 입력 데이터의 간단한 기하학적 변환임

단순한 단계들이 길게 이어져 구현된 신경망을 고차원 공간에서 매우 복잡한 기하학적 변환을 하는 것으로 해석 가능

층을 깊게 쌓으면 아주 복잡한 분해 과정을 처리 가능

# 2.4 신경망의 엔진: 그레디언트 기반 최적화

각층은 입력 데이터를 다음과 같이 변환함

$output = relu(dot(W, input) + b)$

W == 가중치(Weight) 또는 커널(kernel)
b == 훈련되는 파라미터(trainable parameter) 또는 편향(bias)

각 가중치에는 훈련데이터를 모델에 노출시켜 학습된 정보가 담김
$\rightarrow$ 이를 피쳐(feature, 특성)라고 한다.
$\rightarrow$ Fine tuning에서는 이를 이용한다.

1. 초반에는 가중치 행렬이 작은 난수로 채워짐(무작위 초기화(random initialization))
2. 피드백 신호에 기초하여 가중치가 점진적으로 조정(훈련(traing)) $\Rightarrow$  머신러닝의 핵심

**훈련 반복 루프(training loop)**

- 훈련 샘플 x와 이에 상응하는 target y_true를 배치 추출
<br>
- x를 사용해 모델을 실행(정방향 패스(forward pass) 단계), 예측 y_pred를 구함
<br>
- y_pred와 y_true와의 차이를 측정해 이 배치에 대한 손실 계산 : loss
<br>
- 배치에 대한 손실이 조금 감소되도록 모델의 모든 가중치를 업데이트

![[Pasted image 20250305232123.png|500]]
<br>
구식의 방법 : 노가다

==**가중치 행렬의 원소를 모두 고정하고 관심 있는 하나만 다른 값을 적용하는 것**
--> 원소마다 두 번의 큰 비용의 정방향 패스를 계산해야함-> **비효율적**==

더 나은 방법 : 경사하강법 

==미분 가능한 함수를 **그레이티언트(gradient)** 연산==

## 텐서의 연산 도구 : 그레이디언트

텐서의 도함수 == 그레이디언트(gradient)

그레이티언트는 텐서 함수의 곡률(curvature)을 나타냄
$\rightarrow$ 입력 파라미터가 바뀔 때 함수의 출력이 어떻게 바뀌는지 결정함

## 확률적 경사 하강법

- 미분 가능한 함수가 주어지면 이론적으로 이 함수의 최솟값을 해석적으로 구할 수 있음.
<br>
- 신경망에 적용하면 가장 작은 손실 함수의 값을 만드는 가중치의 조합을 해석적으로 찾는 것을 의미함

그레디언트의 반대 방향으로 가중치를 업데이트 하면 손실이 조금씩 감소할 것.

![[Pasted image 20250305231405.png]]

**개선된 훈련 반복 루프 == 미니 배치 확률적 경사 하강법(mini-batch stochastic gradient descent)(미니 배치 SGD)**

- 훈련 샘플 배치 x와 상응하는 target y_true를 추출
<br>
- x로 모델을 실행하고 예측 y_pred를 구함 $\Rightarrow$ 이를 정방향 패스라고 부름
<br>
- 이 배치에서 y_pred와 y_true 사이 오차를 측정하여 모델의 손실(loss)을 계산
<br>
- 모델의 파라미터에 대한 loss function의 gradient를 계산 $\Rightarrow$ 이를 역방향 패스(backward pass)라고 부름
<br>
- 그레이디언트의 반대 방향으로 파라미터를 조금 이동시킴 $\Rightarrow$ 이 과정에서 경사하강법의 속도를 조절하는 스칼라 값을 학습률(learning rate)이라고 함

learning rate를 적절히 고르는 것이 중요함

learning rate가 작으면.... 훈련 반복 횟수가 높아짐, 지역최솟값(local minimum)에 갇힐 수 있음

learning rate가 크면.... 훈련 손실 함수 곡선에서 완전히 임의의 위치로 이동할 수 있음

업데이트할 다음 가중치를 계산할 때 현재 그레이티언트 값 뿐만 아닌 이전 업데이트 가중치를 여러가지 방식으로 고려하는 SGD변종이 있음

![[Pasted image 20250305231521.png|500]]

이를 최적화 방법(optimization method) 또는 옵티마이저(optimizer)라고 함.

**모멘텀(momentum)**은 SGD에 있는 2개의 문제점인 수렵 속도와 지역 최솟값을 해결함


![[Pasted image 20250305224453.png|500]]

그림에서 볼 수 있듯 어떤 파라미터 값에서는 지역 최솟값에 도달함

그 지점에서는 어디를 가도 손실이 증가하지만 사실 전역 최소값은 따로 있기에 최적화되지 않고 지역 최솟값에 갇힘

물리학에서 영감을 받은 모멘텀을 사용하면 이 문제를 타개 가능함

모멘텀은 현재 기울기 값(현재 가속도)뿐만 아니라 (과거의 가속도로 인한) 현재의 속도를 함께 고려하여 각 단계에서 공을 움직임

이를 코드로 구현하면 다음과 같음

```python
past_velocity = 0.
momentum = 0.1
while loss > 0.01:
	w, loss, gradient = get_current_parameters()
	velocity = momentum * velocity - learning_rate * gradient
	past_velocity = velocity
	update_parameter(w)
```

### 도함수의 연결 : 역전파 알고리즘

#연쇄법칙

참고 블로그: 
[8. 연쇄 법칙과 증명 (Chain Rule)](https://vegatrash.tistory.com/17)
내 정리: 
[[연쇄 법칙(Chain rule)]]

역전파 알고리즘은 이전에 설명 했다시피 모델 파라미터에 대한 gradient를 구하는 것임

이때 **연쇄 법칙(chain rule)** 이 사용되며, 연쇄 법칙에 의해 w(가중치)에 대한 도함수를 포함한 여러 층의 gradient를 얻을 수 있음

우선 가정은 다음과 같음: $fg(x) == f(g(x))$

```python
def fg(x):
	x1 = g(x)
	y = f(x1)
	return y
```

여기서 연쇄 법칙을 사용하면 $grad(y, x) == grad(y, x1)*grad(x1, x)$가 성립된다.

따라서 f와 g의 도함수를 알고 있다면 fg의 도함수 역시 계산 가능함 

```python
def fghj(x):
	x1 = j(x)
	x2 = h(x1)
	x2 = h(x2)
	y = f(x3)
	return y
```

위 코드를 수식화 하면 다음과 같다.

$grad(y, x) == (grad(y, x3) * grad(x3, x2) * grad(x2, x1) * grad(x1, x))$

==신경망의 그레이디언트 값을 계산하는데 이 연쇄 법칙을 사용하는 것 $\Rightarrow$ **역전파 알고리즘**==


![[Pasted image 20250306174231.png|500]]

정방향 패스와 역방향 패스에 관한 그림이다.

역전파를 계산 그래프(computation graph)관점에서 생각하면 좋음

유향 비순환 그래프(directed acyclic graph)로 표현된 딥러닝 모델의 정$\cdot$역방향 패스

**정방향 패스**
- 입력 x, 타깃 y_true, w, b에 해당하는 이 그래프의 입력 노드의 값을 설정 
- 이 값을 loss_val에 도달할 때 까지 위에서 아래로 모든 노드에 전파

![[Pasted image 20250306174721.png|350]]

**역방향 패스**
- 모든 A에서 B로 향하는 edge를 반대로 만들고 A가 바뀔 때 B가 얼마나 변하는지 확인
- 아래 그림의 연쇄 법칙 : $grad(loss_val, w) = grad(loss_val, x2) * grad(x2, x1) * grad(x1, w)$

![[Pasted image 20250306175612.png|500]]

위 그래프에서 연쇄 법칙을 적용해서 알 수 있는 값 예 : 
- $grad(loss_val, w) = 1 * 1* 2 = 2$
- $grad(loss_val, w) = 1 * 1 = 1$

==**즉, 계산 그래프에서 각 노드의 손실 기여도를 역전파함 **==

최근에는 텐서플로와 같이 **자동 미분(automatic differentiation)** 이 가능한 프레임워크를 사용해 신경망을 구현

자동 미분은 정방향 패스를 작성하는 것 외에 다른 작업 없이 미분 가능한 텐서 연산의 어떤 조합에 대해서도 gradient 계산 가능함

### 텐서플로와 그레이디언트 테이프

- 텐서플로의 강력한 자동 미분 기능을 활용할 수 있는 API : `GradientTape`
- while 문과 함께 사용하여 ==해당 코드 블록 안의 모든 텐서 연산을 **계산 그래프 형태(Tape)** 로 기록==
- 텐서플로에서는 `tf.Variable` 인스턴스가 그 역할임

```python
import tensorflow as tf

x = tf.Variable(0.) # 초기값 0으로 스칼라 변수를 생성
with tf.GradientTape() as tape: # GradientTape블록을 실행
	y = 2 * x + 3 # 이 블록 안에서 변수에 텐서 연산을 적용
grad_df_y_wrt_x = tape.gradient(y, x) # tape를 사용해서 변수 x에 대한 출력 y의 gradient를 계산
```

GradientTape를 다차원 텐서(rank-$n$ tensor)와 함께 사용 가능

```python
x = tf.Variable(tf.zeros((2, 2)) #크기가 (2, 2), 초기값이 0인 변수 생성
with tf.GradientTape() as tape:
	y = 2 * x + 3
grad_of_y_wrt_x = tape.gradient(y, x) # grad_of_y_wrt_x는 (x와 크기가 같은)(2,2) 크기의 텐서로 x = [[0,0],[0,0]] 일 때 y = 2 * x + 3의 곡률을 나타냄
```

변수 리스트의 Gradient를 계산 할 수도 있음

```python
W = tf.Variable(tf.random.uniform((2, 2)))
b = tf.Variable(tf.zeros((2,)))
x = tf.random.uniform((2, 2))

with tf.GradientTape() as tape:
    y = tf.matmul(x, W) + b # mutmul은 텐서프로의 점곱 함수임
grad_of_y_wrt_W_and_b = tape.gradient(y, [W, b]) # grad_of_y_wrt_W_and_b는 2개의 텐서를 담은 리스트임. 각 텐서는 W, b와 크기가 같음
```

---
## 2.5 첫번째 예제 다시 살펴보기

모델과 층, 손실 함수, 옵티마이저의 관계는 다음 그림과 같다.

![[Pasted image 20250305231642.png|500]]

### 텐서플로를 사용하여 첫번째 예제를 밑바닥부터 다시 구현하기

#### 단순한 Dense 클라스
- Dense 층은 입력 변환을 구현
- W와 b는 모델 파라미터, activation은 각 원소에 적용되는 함수
- 일반적으로 `relu`, 마지막 층에는 `softmax`를 사용


```python
output = activation(dot(W, input) + b)
```

파이썬 클래스 `NaiveDense` 를 구현하여 2개의 텐서플로 변수 W와 b를 만들고 `__call__()` 메서드에서 입력 변환 구현

```python
import tensorflow as tf

class NaiveDense:
	def __init__(self, input_size, output_size, activation):
		self.activation = activation

		w_shape = (input_size, output_size) # 랜덤한 값으로 초기화된 (input_size, output_size) 크기의 행렬 W 만들기
		w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)
		self.W = tf.Variable(w_initial_value)

		b_shape = (output_size,) # 0으로 초기화 된 (outputs_size,)크기의 벡터 b 생성
		b_initial_value = tf.zeros(b_shape)
		self.b = tf.Variable(b_initial_value)

	def __call__(self, inputs): # 정방향패스를 수행 
		return self.activation(tf.matmul(inputs, self.W) + self.b)

	@property
	def weights(self): # 층의 가중치를 추출하기 위한 메서드
		return [self.W, self.b]
```

#### 단순한 Sequential 클라스

- 층의 리스트를 받고 `__call__()` method 에서 입력을 사용하여 층을 순서대로 호출
- 층의 파라미터를 구할 수 있도록 weights 속성 제공

```python
class NaiveSequential:
    def __init__(self, layers):
        self.layers = layers

    def __call__(self, inputs):
        x = inputs
        for layer in self.layers:
           x = layer(x)
        return x

    @property
    def weights(self):
       weights = []
       for layer in self.layers:
           weights += layer.weights
       return weights
```

위 두 개의 class `NaiveDense`와 `NaiveSequential` 을 통해 keras와 유사한 모델 생성 가능

```python
model = NaiveSequential([
	NaiveDense(input_size=28*28, output_size=512, activation=tf.nn.relu),
	NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)
])
assert len(model.weights) == 4
```


#### 배치 제너레이터

MNIST 데이터를 미니 배치로 순회할 방법

```python
import math

class BatchGenerator:
    def __init__(self, images, labels, batch_size=128):
        assert len(images) == len(labels)
        self.index = 0
        self.images = images
        self.labels = labels
        self.batch_size = batch_size
        self.num_batches = math.ceil(len(images) / batch_size)

    def next(self):
        images = self.images[self.index : self.index + self.batch_size]
        labels = self.labels[self.index : self.index + self.batch_size]
        self.index += self.batch_size

        return images, labels
```


### 훈련 스텝 실행

1. 배치에 있는 이미지에 대해 모델의 예측을 계산
2. 실제 레이블을 사용하여 이 예측의 손실 값을 계산
3. 모델 가중치에 대한 손실의 gradient를 계산
4. gradient의 반대 방향으로 가중치를 조금 이동

```python
def one_training_step(model, images_batch, labels_batch):
    with tf.GradientTape() as tape:
        predictions = model(images_batch)
        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(
            labels_batch, predictions)
        average_loss = tf.reduce_mean(per_sample_losses) # 정방향 패스를 실행(GradientTape 블록 안에서 모델의 예측을 계산)
        
    gradients = tape.gradient(average_loss, model.weights) # W에 대한 gradient를 계산. gradients 리스트의 각 항목은 model.weights 리스트에있는 W에 매칭됨 
    update_weights(gradients, model.weights) #  가중치 업데이트

    return average_loss
```

- `update_weights(gradients, model.weights)`"가중치 업데이트' 단계의 목적은 이 배치의 손실을 감소시키기 위한 방향으로 가중치를 '조금' 이동하는 것
- 이동의 크기는 'learning rate(학습률)'에 의해 결정됨
- 학습률은 일반적으로 작은 값
- `update_weights()`함수를 구현하는 방법은 가중치에서 `gradient * learning_rate`를 빼는 것임
```python
def update_weights(gradient, weights):
	for g, w in zip(gradient, weights):
		w.assign_sub(g * learning_rate) # 텐서플로 변수의 assign_sub 메서드는 " -= " 과 동일함
```

물론 실제로는 다음과 같이 케라스의 `Optimizer`인스턴스를 사용
```python
from tensorflow.keras import optimizers

optimizer = optimizers.SGD(learning_rate=1e-3)

def update_weights(gradients, weights):
    optimizer.apply_gradients(zip(gradients, weights))
```
	
### 전체 훈련 루프
- 훈련 에포크 하나는 단순히 훈련 데이터의 각 배치에 대한 훈련 스텝을 반복하는 것.

```python
def fit(model, images, labels, epochs, batch_size=128):
    for epoch_counter in range(epochs):
        print(f"에포크 {epoch_counter}")
        
        batch_generator = BatchGenerator(images, labels)
        
        for batch_counter in range(batch_generator.num_batches):
            images_batch, labels_batch = batch_generator.next()
            loss = one_training_step(model, images_batch, labels_batch)
            
            if batch_counter % 100 == 0:
                print(f"{batch_counter}번째 배치 손실: {loss:.2f}")
```

이제 이 함수를 사용해서 훈련

```python
from tensorflow.keras.datasets import mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype("float32") / 255
test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype("float32") / 255

fit(model, train_images, train_labels, epochs=10, batch_size=128)
```
```txt
에포크 0 
0번째 배치 손실: 5.65 100번째 배치 손실: 2.23 200번째 배치 손실: 2.22 300번째 배치 손실: 2.09 400번째 배치 손실: 2.23 에포크 1 0번째 배치 손실: 1.92 100번째 배치 손실: 1.87 200번째 배치 손실: 1.84 300번째 배치 손실: 1.72 400번째 배치 손실: 1.84 에포크 2 0번째 배치 손실: 1.60 100번째 배치 손실: 1.58 200번째 배치 손실: 1.52 300번째 배치 손실: 1.43 400번째 배치 손실: 1.52 에포크 3 0번째 배치 손실: 1.34 100번째 배치 손실: 1.34 200번째 배치 손실: 1.25 300번째 배치 손실: 1.22 400번째 배치 손실: 1.28 에포크 4

...

100번째 배치 손실: 0.71 200번째 배치 손실: 0.61 300번째 배치 손실: 0.67 400번째 배치 손실: 0.75
```

### 모델 평가하기

- 테스트 이미지에 대한 예측에 `argmax()` 를 적용
- 예상 레이블과 비교 후 모델 평가

```python
predictions = model(test_images)

predictions = predictions.numpy()

predicted_labels = np.argmax(predictions, axis=1)

matches = predicted_labels == test_labels

print(f"정확도: {matches.mean():.2f}")

---
정확도: 0.82
```

---
# 참고 노트 및 자료

[[기획 및 데이터 전처리]]
[[머신러닝 강의 요약]]
![[모두의 딥러닝 (조태호).pdf]]

---
**작성 후기**: <br><br>텐서에 대한 이해와 딥러닝에 사용되는 층 및 함수에 대한 개론이었다. <br><br>가장 기본이 되는 것이기에 반드시 이 부분들을 짚고 넘어가야 한다고 생각한다.<br><br>본문에서는 수학적인 내용이 다소 간추려 나왔지만,<br><br>따로 통계 및 공업수학 통해 깊이를 깊게 가져가면 시너지가 날 것으로 예상된다. 

[^1]: 딥러닝의 수학은 너무나도 방대하기에 수학을 공부하고 사용하는 것 보다는 실전에서 어떤 식으로 코드가 구현되는지, 우선 어떤 상황에서 어떻게 쓰여지고, 왜 이렇게 되는지를 파악하는 것이 적절하다.
