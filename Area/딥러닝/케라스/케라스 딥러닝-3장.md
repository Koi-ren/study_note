---
작성 시작 연월일: 2025-03-07
작성 종료 연월일: 
실습환경: Google - colab
사용 툴:
  - jupyter notebook
  - keras
교재: DEEP LEARNING with Python 2/e, 프랑소와 숄레
교재 진도: 3장
tags:
  - 딥러닝
  - 수학
  - 알고리즘
  - python
  - 모델링
  - 데이터
  - 신경망
  - 케라스
---
[[기획 및 데이터 전처리]]
[[머신러닝 강의 요약]]
[[케라스 딥러닝- 2장]]

---
## 3.1 텐서플로란?

**텐서플로의 핵심 목적 :**
	엔지니어와 연구자가 수치 텐서에 대한 수학적 표현을 적용할 수 있도록 하는 것

- 하나의 플랫폼이고, google과 third party에서 개발하는 많은 구성 요소로 이루어진 거대한 생태계의 중심
- 텐서플로 프로그램은 다른 runtime에 맞게 변환 가능해 실전환경에 쉽게 배포 가능

---
## 3.2 케라스란?

- 케라스는 텐서플로 위에 구축된 파이썬용 딥러닝 API
- 어떤 종류의 딥러닝 모델도 쉽게 만들고 훈련할 수 있는 방법을 제공
- 다양한 하드웨어 환경(CPU, GPU, TPU)에서 실행 가능

![[Pasted image 20250307174906.png|500]]

특징:
- 일관되고 간단한 워크플로 제공
- 실행 가능한 피드백 제공
- 사용자 친화적이기에 높은 생산성

## 3.3 케라스와 텐서플로의 간략한 역사

- 케라스는 원래는 씨아노를 위한 라이브러리였음
- 씨아노는 딥러닝 라이브러리 최초로 자동 미분과 GPU 지원을 제공하는 텐서 조작 라이브러리
- 씨아노가 텐서플로에게 기술 밀림 $\rightarrow$ 텐서플로가 케라스 기본 백엔드가 됨 
## 3.4 딥러닝 작업 환경 설정하기
생략
## 3.5 텐서플로 시작하기

### 상수 텐서와 변수

텐서를 만들려면 초기 값이 필요함
초기 값은 다음과 같이 설정 가능함

```python
>>>import tensorflow as tf
>>>x = tf.ones(shape=(2, 1)) # 내부 원소가 1로 구성된 (2, 1)인 텐서
>>>print(x)

tf.Tensor(
[[1.]
[1.]], shape=(2, 1), dtype=float32)
---
>>>x = tf.zeros(shape=(2, 1)) # 내부 원소가 0으로 구성된 (2, 1)인 텐서
>>>print(x)

tf.Tensor(
[[0.]
[0.]], shape=(2, 1), dtype=float32)
---
>>>x = tf.random.normal(shape=(3,1), mean=0., stddev=1.) # 평균이 0이고 표준편차가 1인 정규분포에서 뽑은 랜덤한 값을로 만든 텐서
>>>x = tf.random.uniform(shape=(3, 1), minval=0., maxval=1.) # 0과 1사이의 균등분포에서 뽑은 랜덤한 값으로 만든 텐서
```

넘파이 배열에서는 직접 값을 할당 할 수 있지만, 텐서플로우에서는 불가능하다.
텐서플로 텐서는 상수이기 때문임

![[Pasted image 20250307231536.png|500]]

따라서 텐서를 업데이트 해야하는 상황에는 변수를 사용해야함
`tf.Variable()`은 텐서플로에서 수정 가능한 상태를 관리하기 위한 클래스임

변수를 만들려면 랜덤 텐서와 같이 초기값을 제공해야 함

```python
>>>v = tf.Variable(initial_value=tf.random.normal(shape=(3, 1)))
>>>print(v)

<tf.Variable 'Variable:0' shape=(3, 1) dtype=float32, numpy= array([[-1.8518561], 
	   [-1.172924 ], 
	   [0.7448435]], dtype=float32)>
```

변수의 상태는 assign 메서드로 수정 가능

```python
>>>v.assign(tf.ones((3, 1)))

<tf.Variable 'UnreadVariable' shape=(3, 1) dtype=float32, numpy=
array([[1.],
       [1.],
       [1.]], dtype=float32)>
```

변수의 일부 원소에만 적용할 수도 있음

```python
v[0, 0].assign(3.)


<tf.Variable 'UnreadVariable' shape=(3, 1) dtype=float32, numpy=
array([[3.],
       [1.],
       [1.]], dtype=float32)>
```

`assign_add()`와 `assign_sub`은 각각 `+=, -= `와 동일

```python
>>>v.assign_add(tf.ones((3, 1)))

<tf.Variable 'UnreadVariable' shape=(3, 1) dtype=float32, numpy=
array([[4.],
       [2.],
       [2.]], dtype=float32)>
```

### 텐서 연산: 텐서플로에서 수학 계산하기
[[케라스 딥러닝- 2장#원소별 연산]]]
[[케라스 딥러닝- 2장#텐서 곱셈]]]

기본적인 수학 연산
```python
a = tf.ones((2, 2))
b = tf.squard(a) # 제곱을 계산
c = tf.sqrt(a) # 제곱근을 계산
d = b + c # 두 텐서를더한다(원소별 연산)
e = tf.matmul(a, b) # 두 텐서의 점곱을 계산
```

중요한 점은 앞의 연산이 넘파이처럼 언제든지 현재 결과값을 출력할 수 있으며, 이를 **즉시 실행(eager execution)** 모드라고 함

### GradientTape API 다시 살펴보기
[[케라스 딥러닝- 2장#텐서플로와 그레이디언트 테이프]]

- 텐서플로는 미분 가능한 표현이면 어떤 입력에도 gradient를 계산 가능

- `GradientTape`블록을 시작하고 하나 또는 여러 입력 텐서에 대한 계산을 수행 후 입력에 대해 결과의 gradient를 구하면 됨

```python
input_var = tf.Variable(initial_value=3.)
with tf.GradientTape() as tape:
  result = tf.square(input_var)
gradient = tape.gradient(result, input_var)
print(gradient)
```

![[Pasted image 20250307234601.png|500]]

- `gradient = tape.gradient(loss, weights)`처럼 가중치에대한 모델 손실의 gradient를 계산하는 대중적인 방법임

- 텐서플로는 기본적으로 훈련 가능한 변수만 추적
상수 텐서의 경우 `tape.watch()`를 호출해 추적함을 수동으로 선언해야함

```python
input_const = tf.constant(3.)
with tf.GradientTape() as tape:
  tape.watch(input_const)
  result = tf.square(input_const)
gradient = tape.gradient(result, input_const)
print(gradient)
```

![[케라스 딥러닝-3장-20250307.png|500]]

#### GradientTape API가 필요한 이유
- 모든 텐서에 대한 모든 gradient를 계산하기 위한 모든 정보를 저장하는 것은 비효율적임
- 자원 낭비를 막기 위해 감시 대상을 미리 탐색해야함
- 훈련 가능한 변수는 기본적으로 감시 대상임
- 훈련 가능한 변수에 대한 loss의 gradient를 계산하는 것이 GradientTape의 주 용도

Gradient API는 second-order(이계도) gradient, 즉 gradient의 gradient도 계산 가능
ex_1 $\rightarrow$ 시간에 대한 물체의 위치의 gradient는 물체의 속도 $v$
ex_1 $\rightarrow$ secon-order gradient는 물체의 가속도 $a$

##### 쉬어갑시다
수직으로 낙하하는 사과의 위치를 시간에 따라 측정하고, $position(time) = 4.9 \times time^2$ 임을 알 떄, 이 사과의 가속도는?

```python
time = tf.Variable(0.)
with tf.GradientTape as outer_tape:
  with tf.GradientTape as inner_tape:
    position = 4.9 * time ** 2
  speed = inner_tape.gradient(position, time)
acceleration = outer_tape.gradient(speed, time)
print(acceleration)
```

![[케라스 딥러닝-3장-20250308.png|500]]

### End to End(e2e) 예제 : 텐서플로 선형 분류기

#### 선형 분류기 제약 조건
- 선형적으로 잘 구분되는 합성 데이터를 생성
- 2D 평면의 포인트로 2개의클래스를 가짐
- 특정한 평균과 공분산 행렬(covariance matrix)을 가진 랜덤한 분포에서 좌표값을 뽑아 각 클래스의 포인트를 생성할 것임
	- 공분산 행렬은 point cloud의 형태를 결정
	- 평균은 평면에서의 위치를 나타냄
- 각 point cloud 에 동일한 공분산 행렬을 사용
- 평균은 다른 값 사용
	- 각 point cloud는 같은 모양이지만 다른 위치에 있을 것임

![[케라스 딥러닝-3장-20250308-1.png|500]]

```python
num_samples_per_class = 1000
# 첫번째 클래스의 포인트 생성 => 1000개의 랜덤한 2D포인트. cov는 왼쪽 아래서 오른쪽 위로 향하는 타원형의 point cloud를 형성
nagative_samples = np.random.multivariate_normal(
	mean=[0, 3],
	cov=[[1, 0.5], [0.5, 1]],
	size=num_samples_per_class)
# 두번째 클래스의포인트 생성 => 첫번쨰와 동일한 공분산 행렬과 다른 평균을 사용하여 다른 클래스의 포인트를 생성 
positive_sample = np.random.multivariate_normal(
	mean=[3, 0],
	cov=[[1, 0.5], [0.5, 1]],
	size=num_samples_per_class)
```

- 각 클래스의 크기는 모두 (1000, 2) 크기의 배열 
- 각 클래스를 수직 연결해 (2000, 2) 크기의 단일 배열화 시킬 것임

```python
inputs = np.vstack((negatice_samples, positive_samples)).astype(np.float32)
```

- (2000, 1)  크기의 0 배열과 1 배열을 합쳐 target label을 생성할 것임
- inputs[i]가 클래스 0에 속하면 targets[i, 0]은 0임
