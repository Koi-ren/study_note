---
작성 시작 연월일: 2025-03-07
작성 종료 연월일: 2025-03-26
실습환경: Google - colab
사용 툴:
  - jupyter notebook
  - keras
교재: DEEP LEARNING with Python 2/e, 프랑소와 숄레
교재 진도: 3장
tags:
  - 딥러닝
  - 수학
  - 알고리즘
  - python
  - 모델링
  - 데이터
  - 신경망
  - 케라스
---
[[기획 및 데이터 전처리]]
[[머신러닝 강의 요약]]
[[케라스 딥러닝 - 2장]]

---
## 3.1 텐서플로란?

**텐서플로의 핵심 목적 :**
	엔지니어와 연구자가 수치 텐서에 대한 수학적 표현을 적용할 수 있도록 하는 것

- 하나의 플랫폼이고, google과 third party에서 개발하는 많은 구성 요소로 이루어진 거대한 생태계의 중심
- 텐서플로 프로그램은 다른 runtime에 맞게 변환 가능해 실전환경에 쉽게 배포 가능

---
## 3.2 케라스란?

- 케라스는 텐서플로 위에 구축된 파이썬용 딥러닝 API
- 어떤 종류의 딥러닝 모델도 쉽게 만들고 훈련할 수 있는 방법을 제공
- 다양한 하드웨어 환경(CPU, GPU, TPU)에서 실행 가능

![[Pasted image 20250307174906.png|500]]

특징:
- 일관되고 간단한 워크플로 제공
- 실행 가능한 피드백 제공
- 사용자 친화적이기에 높은 생산성

## 3.3 케라스와 텐서플로의 간략한 역사

- 케라스는 원래는 씨아노를 위한 라이브러리였음
- 씨아노는 딥러닝 라이브러리 최초로 자동 미분과 GPU 지원을 제공하는 텐서 조작 라이브러리
- 씨아노가 텐서플로에게 기술 밀림 $\rightarrow$ 텐서플로가 케라스 기본 백엔드가 됨 
## 3.4 딥러닝 작업 환경 설정하기
생략
## 3.5 텐서플로 시작하기

### 상수 텐서와 변수

텐서를 만들려면 초기 값이 필요함
초기 값은 다음과 같이 설정 가능함

```python
>>>import tensorflow as tf
>>>x = tf.ones(shape=(2, 1)) # 내부 원소가 1로 구성된 (2, 1)인 텐서
>>>print(x)

tf.Tensor(
[[1.]
[1.]], shape=(2, 1), dtype=float32)
---
>>>x = tf.zeros(shape=(2, 1)) # 내부 원소가 0으로 구성된 (2, 1)인 텐서
>>>print(x)

tf.Tensor(
[[0.]
[0.]], shape=(2, 1), dtype=float32)
---
>>>x = tf.random.normal(shape=(3,1), mean=0., stddev=1.) # 평균이 0이고 표준편차가 1인 정규분포에서 뽑은 랜덤한 값을로 만든 텐서
>>>x = tf.random.uniform(shape=(3, 1), minval=0., maxval=1.) # 0과 1사이의 균등분포에서 뽑은 랜덤한 값으로 만든 텐서
```

넘파이 배열에서는 직접 값을 할당 할 수 있지만, 텐서플로우에서는 불가능하다.
텐서플로 텐서는 상수이기 때문임

![[Pasted image 20250307231536.png|500]]

따라서 텐서를 업데이트 해야하는 상황에는 변수를 사용해야함
`tf.Variable()`은 텐서플로에서 수정 가능한 상태를 관리하기 위한 클래스임

변수를 만들려면 랜덤 텐서와 같이 초기값을 제공해야 함

```python
>>>v = tf.Variable(initial_value=tf.random.normal(shape=(3, 1)))
>>>print(v)

<tf.Variable 'Variable:0' shape=(3, 1) dtype=float32, numpy= array([[-1.8518561], 
	   [-1.172924 ], 
	   [0.7448435]], dtype=float32)>
```

변수의 상태는 assign 메서드로 수정 가능

```python
>>>v.assign(tf.ones((3, 1)))

<tf.Variable 'UnreadVariable' shape=(3, 1) dtype=float32, numpy=
array([[1.],
       [1.],
       [1.]], dtype=float32)>
```

변수의 일부 원소에만 적용할 수도 있음

```python
v[0, 0].assign(3.)


<tf.Variable 'UnreadVariable' shape=(3, 1) dtype=float32, numpy=
array([[3.],
       [1.],
       [1.]], dtype=float32)>
```

`assign_add()`와 `assign_sub`은 각각 `+=, -= `와 동일

```python
>>>v.assign_add(tf.ones((3, 1)))

<tf.Variable 'UnreadVariable' shape=(3, 1) dtype=float32, numpy=
array([[4.],
       [2.],
       [2.]], dtype=float32)>
```

### 텐서 연산: 텐서플로에서 수학 계산하기
[[케라스 딥러닝 - 2장#원소별 연산]]]
[[케라스 딥러닝 - 2장#텐서 곱셈]]]

기본적인 수학 연산
```python
a = tf.ones((2, 2))
b = tf.squard(a) # 제곱을 계산
c = tf.sqrt(a) # 제곱근을 계산
d = b + c # 두 텐서를더한다(원소별 연산)
e = tf.matmul(a, b) # 두 텐서의 점곱을 계산
```

중요한 점은 앞의 연산이 넘파이처럼 언제든지 현재 결과값을 출력할 수 있으며, 이를 **즉시 실행(eager execution)** 모드라고 함

### GradientTape API 다시 살펴보기
[[케라스 딥러닝 - 2장#텐서플로와 그레이디언트 테이프]]

- 텐서플로는 미분 가능한 표현이면 어떤 입력에도 gradient를 계산 가능

- `GradientTape`블록을 시작하고 하나 또는 여러 입력 텐서에 대한 계산을 수행 후 입력에 대해 결과의 gradient를 구하면 됨

```python
input_var = tf.Variable(initial_value=3.)
with tf.GradientTape() as tape:
  result = tf.square(input_var)
gradient = tape.gradient(result, input_var)
print(gradient)
```

![[Pasted image 20250307234601.png|500]]

- `gradient = tape.gradient(loss, weights)`처럼 가중치에대한 모델 손실의 gradient를 계산하는 대중적인 방법임

- 텐서플로는 기본적으로 훈련 가능한 변수만 추적
상수 텐서의 경우 `tape.watch()`를 호출해 추적함을 수동으로 선언해야함

```python
input_const = tf.constant(3.)
with tf.GradientTape() as tape:
  tape.watch(input_const)
  result = tf.square(input_const)
gradient = tape.gradient(result, input_const)
print(gradient)
```

![[케라스 딥러닝-3장-20250307.png|500]]

#### GradientTape API가 필요한 이유
- 모든 텐서에 대한 모든 gradient를 계산하기 위한 모든 정보를 저장하는 것은 비효율적임
- 자원 낭비를 막기 위해 감시 대상을 미리 탐색해야함
- 훈련 가능한 변수는 기본적으로 감시 대상임
- 훈련 가능한 변수에 대한 loss의 gradient를 계산하는 것이 GradientTape의 주 용도

Gradient API는 second-order(이계도) gradient, 즉 gradient의 gradient도 계산 가능
ex_1 $\rightarrow$ 시간에 대한 물체의 위치의 gradient는 물체의 속도 $v$
ex_1 $\rightarrow$ secon-order gradient는 물체의 가속도 $a$

##### 쉬어갑시다
수직으로 낙하하는 사과의 위치를 시간에 따라 측정하고, $position(time) = 4.9 \times time^2$ 임을 알 떄, 이 사과의 가속도는?

```python
time = tf.Variable(0.)
with tf.GradientTape as outer_tape:
  with tf.GradientTape as inner_tape:
    position = 4.9 * time ** 2
  speed = inner_tape.gradient(position, time)
acceleration = outer_tape.gradient(speed, time)
print(acceleration)
```

![[케라스 딥러닝-3장-20250308.png|500]]

### End to End(e2e) 예제 : 텐서플로 선형 분류기

#### 선형 분류기 제약 조건
- 선형적으로 잘 구분되는 합성 데이터를 생성
- 2D 평면의 포인트로 2개의클래스를 가짐
- 특정한 평균과 공분산 행렬(covariance matrix)을 가진 랜덤한 분포에서 좌표값을 뽑아 각 클래스의 포인트를 생성할 것임
	- 공분산 행렬은 point cloud의 형태를 결정
	- 평균은 평면에서의 위치를 나타냄
- 각 point cloud 에 동일한 공분산 행렬을 사용
- 평균은 다른 값 사용
	- 각 point cloud는 같은 모양이지만 다른 위치에 있을 것임

![[케라스 딥러닝-3장-20250308-1.png|500]]

```python
num_samples_per_class = 1000 # 첫번째 클래스의 포인트 생성 => 1000개의 랜덤한 2D포인트. cov는 왼쪽 아래서 오른쪽 위로 향하는 타원형의 point cloud를 형성

negative_samples = np.random.multivariate_normal(

    mean=[0, 3],

    cov=[[1, 0.5],[0.5, 1]],

    size=num_samples_per_class) # 두번째 클래스의포인트 생성 => 첫번쨰와 동일한 공분산 행렬과 다른 평균을 사용하여 다른 클래스의 포인트를 생성

positive_samples = np.random.multivariate_normal(

    mean=[3, 0],

    cov=[[1, 0.5],[0.5, 1]],

    size=num_samples_per_class)
```

- 각 클래스의 크기는 모두 (1000, 2) 크기의 배열 
- 각 클래스를 수직 연결해 (2000, 2) 크기의 단일 배열화 시킬 것임

```python
inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)
```

- (2000, 1)  크기의 0 배열과 1 배열을 합쳐 target label을 생성할 것임
- `inputs[i]`가 클래스 0에 속하면 `targets[i, 0]`은 0임

```python
targets = np.vstack((np.zeros((num_samples_per_class, 1), dtype="float32"),

                     np.ones((num_samples_per_class, 1), dtype="float32")))
```

**시각화**

```python
import matplotlib.pyplot as plt

plt.scatter(inputs[:, 0], inputs[:, 1], c=targets[:, 0])

plt.show()
```

![[케라스 딥러닝-3장-20250308-2.png|500]]

이제 각 point cloud를 구별할 수 있는 선형 분류기를 만들 것임
- 선형 분류기는 하나의 아핀 변환[^1]($prediction = W \cdot input + b$)임
- 예측과 타깃 사이의 차이를 제곱한 값을 최소화 하도록 훈련

각각 랜덤한 값과 0으로 초기화한  가중치 $W$와 모델 파라미터 $b$를 설정
```python
input_dim = 2 # 입력은 2D point

output_dim = 1 # 출력 예측은 샘플당 하나의 점수 (0에 가까우면 샘플을 클래스 0으로 예측, 1에 가까우면 클래스 1로 예측)

W = tf.Variable(initial_value=tf.random.uniform(shape=(input_dim, output_dim)))

b = tf.Variable(initial_value=tf.zeros(shape=(output_dim,)))
```

**정방향 패스(forward pass)를 위한 function**
```python
def model(inputs):

    return tf.matmul(inputs, W) + b  # mutmul은 텐서플로의 점곱 함수
```

- 이 선형 분류기는 2D입력을 다룸
$\rightarrow$ $W$는 2개의스칼라 가중치 $w1$과 $w2$로 이루어짐($W = w1 + w2$)
- $b$는 스칼라 값
- 어떤 입력 포인트 `[x, y]`가 주어지면 예측 값은 다음과 같이 됨
$\rightarrow prediction = [[w1], [w2] \cdot [x, y] + b = (w1 \times x) + (w2 \times y) + b$]

**loss function (MSE)**
```python
def square_loss(targets, predictions):

    per_sample_losses = tf.square(targets - predictions) #per_sample_losses는 targests나 predictions와 크기가 같은 텐서이며 각 샘플의 손실 값을 담고 있음

    return tf.reduce_mean(per_sample_losses) # reduce_mean()가 샘플당 손실 값을 하나의 스칼라 손실 값으로 평균
```

훈련 스텝으로 훈련 데이터를 받아 이 데이터에 대한 손실을 최소화하도록 $weight(W)$ 와 모델 파라미터 ($b$)를 업데이트
```python
learning_rate = 0.1

  

def training_step(inputs, targets): # 그레디언트 테이프 블록 안의 정방향 패스

    with tf.GradientTape() as tape:

        predictions = model(inputs)

        loss = square_loss(targets, predictions)

    grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b]) #가중치에 대한 손실의 그레디언트를 구함

    W.assign_sub(grad_loss_wrt_W * learning_rate)

    b.assign_sub(grad_loss_wrt_b * learning_rate)

    return loss
```

- 배치 훈련 사용
	- 데이터를 작은 배치로 나누어 반복하지 않고 전체 데이터를 사용하여 훈련 스텝 (그레이디언트 계산과 가중치 업데이트를 실행))
- 2000개 샘플에 대해 정방향 패스와 그레이디언트를 계산
	- 훈련 시간 늘어남
- 128개의 랜덤한 샘플을 사용하지 않고 전체 훈련 샘플로부터 정보를 취합
	- 각각의 그레이디언트 업데이트는 훈련 데이터의 손실을 감소하는 데 훨씬 더 효과적

결과적으로  훈련 스텝의 횟수가 많이 필요하지 않음
미니 배치 훈련 떄보다 일반적으로 큰 학습률을 사용 가능(위 코드의 learning_rate (learning_rate = 0.1)을 이용 )

```python
for step in range(40):

    loss = training_step(inputs, targets)

    print(f"{step}번째 스텝의 손실: {loss:.4f}")
```

![[케라스 딥러닝-3장-20250311.png|500|300]]

- 40번 에포크 후 훈련 손실이 0.0260으로 안정화됨

훈련 입력에 대한 모델의 예측 : 훈련 타깃과 매우 유사함  

```python
predictions = model(inputs)

plt.scatter(inputs[:,0], inputs[:, 1], c=predictions[:, 0]>0.5)

plt.show()
```
![[케라스 딥러닝-3장-20250311-1.png|500]]

포인트 `[x, y]`에 대한 예측값
- `prediction == [[w1], [w2]]`$\cdot$`[x, y] + b == w1 * x + w2 * y +b
- `class 0 == w1 * x + w2 * y + b < 0.5`
- `class 1 == w1  *  x + w2 * y + b > 0.5`

따라서 `class 0`과 `class 1`을 구분 짓는 것은 직선의 방정식
- `직선의 방정식 == w1  *  x + w2 * y + b = 0.5`
- `정규화 방정식 == - w1 / w2 *x + (0.5 - b) / w2`

이를 코드화 시키면 다음과 같음
```python
x = np.linspace(-1, 4, 100)

# 사실 100개의 x 축 좌표를 만들 필요 없이 시작과 종료 위치만 있어도 됨.
# 따라서 x = np.linspace(-1, 4, 100) 대신 x = [-1, 4] 를 사용해도 무방함

y = - W[0] /  W[1] * x + (0.5 - b) / W[1]

plt.plot(x, y, "-r")

plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] > 0.5)

plt.show()
```

![[케라스 딥러닝-3장-20250311-2.png|500]]

데이터에 있는 두클래스를 잘구분하는 직선(또는 고차원 공간의 경우 초평면(hyperplane))의 파라미터를 찾는 것 $\rightarrow$ 선형 분류기  

## 3.6 신경망의 구조 : 핵심 Keras API 이해
### 층 : 딥러닝의 구성 요소
- 신경망의 기본 데이터 구조는 2장에서 소개한 층(layers)
- 층은 하나 이상의 텐서를 출력하는 데이터 처리모듈
- 어떤 종류의 층은 상태가 없지만 대부분의 경우 $Weight$라는 층의 상태를 가짐
- $Weight$는 확률적 경사 하강법으로 학습되는 하나 이상의 텐서
	- 가중치와 모델 파라미터는 딥러닝을 통해 모델이 학습한 결과물임

layer마다 적절한 tensor format과 data processing이 다르다
- **밀집 연결 층(densely connected layer) == 완전 연결 층(fully connected layer) == 밀집 층(dense layer)**
	- (sample, features)크기의 rank-2 tensor의 벡터 데이터
- **순환 층(recurrent layer)[^2] or 1D 합성곱 층(convolution layer)(Conv1D)**
	- (sample, timesteps, features) 크기의 rank-3 tensor의 시퀸스 데이터
	#LSTM #layer
- **2D 합성곱 층(Conv2D)**
	- rank-4 tensor의 이미지 데이터
#### 케라스의 layer 클래스
- 간단한 API는 모든 것이 중심에 모인 하나의 추상화를 가져야 함
	- 케라스에서 Layer 클래스가 대표적인 예
	- 케라스에서는 Layer 또는 Layer와 밀접하게 상호 작용하는 것이 전부

Layer는 상태(Weight)와 연산(forward pass)을 캡슐화한 객체임
[[객체 지향 프로그래밍]]
- Weight(가중치)는 일반적으로 `build()` 메서드에서 정의 
- 연산은 `call()`에서 정의
- 두가지 모두 `__init__()`메서드에서 만들 수도 있지만 일반적으로는 위와 같이 정의함 

2장에서 2개의 W와 b를가지고 `output = activation(dot(input, W) + b)`계산을 수행하는 NaiveDense클래스를 정의했음
[[케라스 딥러닝 - 2장#단순한 Dense 클라스]]

다음 코드는 이와 동일한 케라스 층 코드임
```python
from tensorflow import keras

  

class SimpleDense(keras.layers.Layer): # 모든 케라스 층은 Layer 클래스를 상속해야함

  def __init__(self, units, activation=None):

    super().__init()

    self.units = units

    self.activation = activation

  

def build(self, input_shape): # build() 메서드에서 가중치를 생성

  input_dim = input_shape[-1] # 입력 텐서의 rank 정의

  self.W = self.add_weight(shape=(input_dim, self.units), initializer="random_normal") # add_weight()는 가중치를 간편하게 만들 수 있는 메서드임 self.W = tf.Variable(uniform(w_shape))와 같이 독립적으로 변수를 생성하고 층의 속성으로 할당 가능

  self.b = self.add_weight(shape=(self.units,), initializer="zeros")

  

def call(self, inputs): # call() 메서드에서 정방향 패스 계산을 정의

  y = tf.matmul(inputs, self.W) + self.b

  if self.activation is not None:

    y = self.activation(y)

  return y
```

이 클래스의 인스턴스를 생성하면 텐서플로 텐서를 입력으로 받는 함수로 사용 가능

```python
>>my_dense = SimpleDense(units=32, activation=tf.nn.relu)
>>input_tensor =tf.ones(shape=(2, 784))
>>output_tensor = my_dense(input_tensor)
>>print(output_tensor.shape)

(2, 32)
```

`__call__()`메소드로 호출했는데 `call(), build()`메서드를 구현한 이유
$\rightarrow$ 때에 맞추어 가중치를 생성해야 하기 때문임

### **자동 크기 추론 : 동적으로 층 만들기**
**층 호환(layer compatibility)**: 호환되는 층만 서로 연결하는 것
- 층 호환이란?
	1. 모든 층이 특정 크기의 입력 텐서만 받음
	2. 모든 층이 특정 크기의 출력 텐서만 반환함
```python
# 3 - 1
import tensorflow.keras.layers as layers
layer = layers.Dense(32, activation="relu")
```

- `3 - 1` 층은 첫 번째 차원이 32인 텐서를 반환함
- 입력으로 32 차원인 벡터를 기대하는 후속 층에만 연결 가능(layer compatibilty  때문)

케라스에서는 대부분의 경우 크기 호환성에 대해 걱정할 필요 없음
- 모델에 추가하는 층은 앞선 층의 크기에 맞도록 동적으로 만들어 짐

자동 크기 추론은 다음과 같이 사용됨
```python
model = keras.Sequential([
	simpleDense(32, activation="relu")
	simpleDense(64, activation="relu")
	simpleDense(32, activation="relu")
	simpleDense(10, activation="softmax")
])
```
### **층에서 모델로**
- 딥러닝 모델은 층으로 구성된 그래프
- 케라스에서는 Model 클래스에 해당

자주 등장하는 딥러닝 모델 구조
- 2개 가지(two-branch)를 가진 네트워크
- 멀티헤드(multihead) 네트워크
- 잔차 연결(residual connection)

케라스에서 복잡한 모델을 만드는 방법은 두 가지
1. 직접 Model 클래스의 서브클래스를 만들기
2. 더 적은 코드로 많은 일을 할 수 있는 함수형 API의 사용 $\rightarrow$ [[케라스 딥러닝 - 7장]]

**모델의 구조는 가설 공간(hypothesis space)을 정의**
- 머신러닝 : 사전에 정의된 **가능성 있는 공간(space of possibility)**안에서 피드백 신호의 도움을 받아 입력 데이터의 유용한 표현을 찾는 것
- 네트워크 구조를 선택하면 가능성 있는 공간(가설 공간)이 입력 데이터를 출력 데이터로 매핑하는 일련의 특정한 텐서 연산으로 제한됨

학습에서 데이터에 대한 가정은 이런 가정이 학습할 수 있는 것을 정의함

따라서 ==**가설 공간의 구조**, 즉 **모델의 구조**는 매우 중요함==

문제에 대한 가정: 시작할 때 모델이 가지게 될 사전 지식 인코딩

### **`compile()`: 학습 과정 설정**

모델의 구조 정의 후 다음 세 가지를 더 선택해야 함
- **손실 함수(loss function)**: 훈련 과정에서 최소화할 값
- **옵티마이저(optimizer)**: 손실 함수를 기반으로 네트워크가 어떻게 업데이트될지 결정
- **측정 지표**: 훈련과 검증 과정에서 모니터링할 성공의 척도
위 선택은 `compile()`을 통해 선언 가능
```python
model = keras.Sequential([keras.layers.Dense(1)]) # 선형 분류기 정의
model.compile(optimizer="rmsprop", # 옵티마이저 이름 지정(대소문자 구분 없음)
			  loss="mean_squered_error", # 손실 이름을 지정
			  metrics=["accuracy"]) # 측정 자료를 리스트로 지정	
```

위 예시는 단축어가 아닌 파이썬 객체처럼 매개변수를 인스턴스 객체로 지정 가능
아래의 코드가 바로 그것임

```python
model.compile(optimizer=keras.optimizers.RMSprop(), # 옵티마이저 이름 지정(대소문자 구분 없음)
			  loss=keras.losses.MeanSquaredError(), # 손실 이름을 지정
			  metrics=[keras.metrics.BinaryAccuracy()]) # 측정 자료를 리스트로 지정
```


|     | optimizer                                                   | loss function                                                                                                                                      | 측정 지표                                                                                                         |
| --- | ----------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------- |
| 종류  | - SGD(모멘텀 선택 가능)<br>- RMSprop<br>- Adam<br>- Adgrad<br>- 그외 | - CategoricalCrossentropy<br>- SparseCategoricalCrossentropy<br>- BinaryCrossentropy<br>- MeanSquaredError<br>- KLDivergence<br>- CosineSimilarity | - CategoricalAccuracy<br>- SparseCategorical<br>- BinaryAccuracy<br>- AUC<br>- precision<br>- Recall<br>- 그 외 |


### **손실 함수 선택하기**
- 문제에 맞는 올바른 손실 함수를 선택하는 것은 중요
	- 네트워크가 손실을 최소화 하기 위해 편법을 사용할 수도 있기 때문임

- 손실 함수가 적절하지 않으면  원치 않는 부수 효과가 발생할 것
- 가이드 라인이 존재함
	- 이진 크로스엔트로피(binary crossentropy) : 2개의 클래스가 있는 분류 문제
	- 범주형 크로스엔트로피(categorical crossentropy) : 여러 개의 클래스가 있는 분류 문제

- 완전히 새로운 연구를 할 때만 자신만의 손실 함수를 만들게 될 것

### **`fit()`메서드 이해하기**

 `compile()`다음에는 `fit()` 메서드 호출
 `fit()`메서드는 훈련 루프를 구현

- `fit()`메서드의 주요 매개변수
	- ==훈련할 데이터(input과 outputs)== :
		- 일반적으로 넘파이 배열이나 텐서플로 Dataset 객체로 전달
	- ==훈련할 에포크(epoch) 횟수== :
		- 전달한 데이터에서 훈련 루프를 몇 번 반복할 것인지 선언
	- ==미니 배치 경사 하강법의 각 에포크에서 사용할 배치 크기== :
		- 가중치 업데이트 단계에서 gradient 계산에 사용될 훈련 샘플 개수 선언

```python
history = model.fit(
	inputs, # 입력 샘플
	targets, #훈련 타겟
	epochs=5, # 훈련 루프를 다섯 번 반복
	batch_size=128 # 훈련 루프는 128개의 샘플 배치로 이 데이터를 순회 
)
```

`fit()`을 호출하면 `History`객체가 반환됨
- `History`객체는 `history`속성을 가지고 있음
이 딕셔너리는  "loss"또는 특정 측정 지표 이름의 키와 각 에포크 값의 리스트를 매핑함


```python
>>>history.history

{'loss': [10.188525199890137,
  9.814451217651367,
  9.498977661132812,
  9.191787719726562,
  8.891997337341309],
 'binary_accuracy': [0.0020000000949949026,
  0.0020000000949949026,
  0.0020000000949949026,
  0.0020000000949949026,
  0.0020000000949949026]}
```

### **검증 데이터에서 손실과 측정 지표 모니터링하기**

머신 러닝의 목표는 훈련 데이터에서 잘 동작하는 모델을 얻는 것이 아닌, ==범용적으로 잘 동작하는 모델을 얻는 것임==

**검증 데이터(validation data)** : 새로운 데이터에 모델이 어떻게 동작하는지 예상하기 위해 훈련 데이터의 일부를 떼어 놓는 일반적인 방법

검증 데이터로 모델을 훈련하지는 않지만, 손실과 측정 지표를 계산
- `fit()`메서드의 `validation_data` 매개변수를 사용

```python
model = keras.Sequential([keras.layers.Dense(1)])
model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.1),
              loss=keras.losses.MeanSquaredError(),
              metrics=[keras.metrics.BinaryAccuracy()])

# 검증 데이터에 한 클래스의 샘플만 포함되는 것을 막기 위해 랜덤하게 생성한 인덱스를 사용해서 입력과 타겟을 섞음
indices_permutation = np.random.permutation(len(inputs))
shuffled_inputs = inputs[indices_permutation]
shuffled_targets = targets[indices_permutation]

# 훈련 입력과 타겟의 30%를 검증용으로 떼어놓음(검증 손실과 측정 지표 계산을 위해 훈련 데이터에서 이 샘플을 제외 후 따로 보관)
num_validation_samples = int(0.3 * len(inputs))
val_inputs = shuffled_inputs[:num_validation_samples]
val_targets = shuffled_targets[:num_validation_samples]
training_inputs = shuffled_inputs[num_validation_samples:]
training_targets = shuffled_targets[num_validation_samples:]
model.fit(
    training_inputs,
    training_targets,
    epochs=5,
    batch_size=16,
    validation_data=(val_inputs, val_targets)
)
```

검증 데이터의 손실 값을 '훈련 손실(training loss)'과 구분하기 위해 '검증 손실(validation loss)'이라고 명명함

검증 목적은 모델이 학습한 것이 새로운 데이터에 실제로 유용한지 모니터링하기 위함
따라서 훈련 데이터에 검증 데이터가 노출되면 검증 손실과 측정 지표가 오염되어 모니터링이 무의미해짐

훈련 종료 후 검증 손실과 측정 지표 계산하는 법 : `evaluate()`메서드 사용
```python
loss_and_metrics = model.evaluate(val_inputs, val_targets, batch_size=128)
```

`evaluate()`메서드는 전달된 데이터를 (batch_size 크기의) 배치로 순회하고 스칼라 값의 리스트를 반환함

반환된 리스트의 첫 번째 항목은 검증 손실, 두 번째는 검증 데이터에 대한 측정 지표임

모델에 측정 지표를 지정하지 않으면 (리스트가 아닌) 검증 손실만 반환됨

### **추론 : 훈련한 모델 사용하기**

**추론(ingerence)** : 모델을 훈련하고 이 모델을 사용해 새로운 데이터에서 예측을 하는 것

추론을 사용하는 간단한 방법은 `__call()__()`메서드를 호출 하는 것임

```python
# 넘파이 배열이나 텐서플로 텐서를 받고 텐서플로 텐서를 반환
predictions = model(new_inputs)
```

- 위 방법은 `new_inputs`의 모든 입력을 한 번에 처리
- 데이터가 많으면 불가능 할 수도 있음

추론을 하는 더 나은 방법은 `predict()`메서드를 사용하는 것임
- 데이터를 작은 배치로 순회하여 넘파이 배열로 예측을 반환
- `__call__()`과 달리 텐서플로 Dataset 객체도 처리 가능

```python
# 넘파이 배열이나 Dataset객체를 받고 넘파이 배열을 반환
>>> predictions = model.predict(new_inputs, batch_size=128)
>>> print(predictions[:10])

5/5 [==============================] - 0s 3ms/step
[[0.2212547 ]
 [0.46757108]
 [1.3064125 ]
 [0.36760795]
 [0.98762196]
 [1.2589831 ]
 [0.9364893 ]
 [0.2468108 ]
 [0.94096804]
 [0.4054407 ]]

```


---
**작성 후기**: 

앞으로 배울 딥러닝을 모두 꿰뚫는 가장 기본이 되는 지식이다

지난 2장이 딥러닝의 기초 지식과 교양이었다면 이것은 대략적인 플로우를 설명하였다

이것을 토대로 앞으로 나아가야 할 것이다

[^1]: [[케라스 딥러닝 - 2장#텐서연산의 기하학적 해석]]

[^2]: LSTM 에서 주로 사용된다
