---
작성 시작 연월일: 2025-03-07
작성 종료 연월일: 
실습환경: Google - colab
사용 툴:
  - jupyter notebook
  - keras
교재: DEEP LEARNING with Python 2/e, 프랑소와 숄레
교재 진도: 3장
tags:
  - 딥러닝
  - 수학
  - 알고리즘
  - python
  - 모델링
  - 데이터
  - 신경망
  - 케라스
---
[[기획 및 데이터 전처리]]
[[머신러닝 강의 요약]]
[[케라스 딥러닝 - 2장]]

---
## 3.1 텐서플로란?

**텐서플로의 핵심 목적 :**
	엔지니어와 연구자가 수치 텐서에 대한 수학적 표현을 적용할 수 있도록 하는 것

- 하나의 플랫폼이고, google과 third party에서 개발하는 많은 구성 요소로 이루어진 거대한 생태계의 중심
- 텐서플로 프로그램은 다른 runtime에 맞게 변환 가능해 실전환경에 쉽게 배포 가능

---
## 3.2 케라스란?

- 케라스는 텐서플로 위에 구축된 파이썬용 딥러닝 API
- 어떤 종류의 딥러닝 모델도 쉽게 만들고 훈련할 수 있는 방법을 제공
- 다양한 하드웨어 환경(CPU, GPU, TPU)에서 실행 가능

![[Pasted image 20250307174906.png|500]]

특징:
- 일관되고 간단한 워크플로 제공
- 실행 가능한 피드백 제공
- 사용자 친화적이기에 높은 생산성

## 3.3 케라스와 텐서플로의 간략한 역사

- 케라스는 원래는 씨아노를 위한 라이브러리였음
- 씨아노는 딥러닝 라이브러리 최초로 자동 미분과 GPU 지원을 제공하는 텐서 조작 라이브러리
- 씨아노가 텐서플로에게 기술 밀림 $\rightarrow$ 텐서플로가 케라스 기본 백엔드가 됨 
## 3.4 딥러닝 작업 환경 설정하기
생략
## 3.5 텐서플로 시작하기

### 상수 텐서와 변수

텐서를 만들려면 초기 값이 필요함
초기 값은 다음과 같이 설정 가능함

```python
>>>import tensorflow as tf
>>>x = tf.ones(shape=(2, 1)) # 내부 원소가 1로 구성된 (2, 1)인 텐서
>>>print(x)

tf.Tensor(
[[1.]
[1.]], shape=(2, 1), dtype=float32)
---
>>>x = tf.zeros(shape=(2, 1)) # 내부 원소가 0으로 구성된 (2, 1)인 텐서
>>>print(x)

tf.Tensor(
[[0.]
[0.]], shape=(2, 1), dtype=float32)
---
>>>x = tf.random.normal(shape=(3,1), mean=0., stddev=1.) # 평균이 0이고 표준편차가 1인 정규분포에서 뽑은 랜덤한 값을로 만든 텐서
>>>x = tf.random.uniform(shape=(3, 1), minval=0., maxval=1.) # 0과 1사이의 균등분포에서 뽑은 랜덤한 값으로 만든 텐서
```

넘파이 배열에서는 직접 값을 할당 할 수 있지만, 텐서플로우에서는 불가능하다.
텐서플로 텐서는 상수이기 때문임

![[Pasted image 20250307231536.png|500]]

따라서 텐서를 업데이트 해야하는 상황에는 변수를 사용해야함
`tf.Variable()`은 텐서플로에서 수정 가능한 상태를 관리하기 위한 클래스임

변수를 만들려면 랜덤 텐서와 같이 초기값을 제공해야 함

```python
>>>v = tf.Variable(initial_value=tf.random.normal(shape=(3, 1)))
>>>print(v)

<tf.Variable 'Variable:0' shape=(3, 1) dtype=float32, numpy= array([[-1.8518561], 
	   [-1.172924 ], 
	   [0.7448435]], dtype=float32)>
```

변수의 상태는 assign 메서드로 수정 가능

```python
>>>v.assign(tf.ones((3, 1)))

<tf.Variable 'UnreadVariable' shape=(3, 1) dtype=float32, numpy=
array([[1.],
       [1.],
       [1.]], dtype=float32)>
```

변수의 일부 원소에만 적용할 수도 있음

```python
v[0, 0].assign(3.)


<tf.Variable 'UnreadVariable' shape=(3, 1) dtype=float32, numpy=
array([[3.],
       [1.],
       [1.]], dtype=float32)>
```

`assign_add()`와 `assign_sub`은 각각 `+=, -= `와 동일

```python
>>>v.assign_add(tf.ones((3, 1)))

<tf.Variable 'UnreadVariable' shape=(3, 1) dtype=float32, numpy=
array([[4.],
       [2.],
       [2.]], dtype=float32)>
```

### 텐서 연산: 텐서플로에서 수학 계산하기
[[케라스 딥러닝 - 2장#원소별 연산]]]
[[케라스 딥러닝 - 2장#텐서 곱셈]]]

기본적인 수학 연산
```python
a = tf.ones((2, 2))
b = tf.squard(a) # 제곱을 계산
c = tf.sqrt(a) # 제곱근을 계산
d = b + c # 두 텐서를더한다(원소별 연산)
e = tf.matmul(a, b) # 두 텐서의 점곱을 계산
```

중요한 점은 앞의 연산이 넘파이처럼 언제든지 현재 결과값을 출력할 수 있으며, 이를 **즉시 실행(eager execution)** 모드라고 함

### GradientTape API 다시 살펴보기
[[케라스 딥러닝 - 2장#텐서플로와 그레이디언트 테이프]]

- 텐서플로는 미분 가능한 표현이면 어떤 입력에도 gradient를 계산 가능

- `GradientTape`블록을 시작하고 하나 또는 여러 입력 텐서에 대한 계산을 수행 후 입력에 대해 결과의 gradient를 구하면 됨

```python
input_var = tf.Variable(initial_value=3.)
with tf.GradientTape() as tape:
  result = tf.square(input_var)
gradient = tape.gradient(result, input_var)
print(gradient)
```

![[Pasted image 20250307234601.png|500]]

- `gradient = tape.gradient(loss, weights)`처럼 가중치에대한 모델 손실의 gradient를 계산하는 대중적인 방법임

- 텐서플로는 기본적으로 훈련 가능한 변수만 추적
상수 텐서의 경우 `tape.watch()`를 호출해 추적함을 수동으로 선언해야함

```python
input_const = tf.constant(3.)
with tf.GradientTape() as tape:
  tape.watch(input_const)
  result = tf.square(input_const)
gradient = tape.gradient(result, input_const)
print(gradient)
```

![[케라스 딥러닝-3장-20250307.png|500]]

#### GradientTape API가 필요한 이유
- 모든 텐서에 대한 모든 gradient를 계산하기 위한 모든 정보를 저장하는 것은 비효율적임
- 자원 낭비를 막기 위해 감시 대상을 미리 탐색해야함
- 훈련 가능한 변수는 기본적으로 감시 대상임
- 훈련 가능한 변수에 대한 loss의 gradient를 계산하는 것이 GradientTape의 주 용도

Gradient API는 second-order(이계도) gradient, 즉 gradient의 gradient도 계산 가능
ex_1 $\rightarrow$ 시간에 대한 물체의 위치의 gradient는 물체의 속도 $v$
ex_1 $\rightarrow$ secon-order gradient는 물체의 가속도 $a$

##### 쉬어갑시다
수직으로 낙하하는 사과의 위치를 시간에 따라 측정하고, $position(time) = 4.9 \times time^2$ 임을 알 떄, 이 사과의 가속도는?

```python
time = tf.Variable(0.)
with tf.GradientTape as outer_tape:
  with tf.GradientTape as inner_tape:
    position = 4.9 * time ** 2
  speed = inner_tape.gradient(position, time)
acceleration = outer_tape.gradient(speed, time)
print(acceleration)
```

![[케라스 딥러닝-3장-20250308.png|500]]

### End to End(e2e) 예제 : 텐서플로 선형 분류기

#### 선형 분류기 제약 조건
- 선형적으로 잘 구분되는 합성 데이터를 생성
- 2D 평면의 포인트로 2개의클래스를 가짐
- 특정한 평균과 공분산 행렬(covariance matrix)을 가진 랜덤한 분포에서 좌표값을 뽑아 각 클래스의 포인트를 생성할 것임
	- 공분산 행렬은 point cloud의 형태를 결정
	- 평균은 평면에서의 위치를 나타냄
- 각 point cloud 에 동일한 공분산 행렬을 사용
- 평균은 다른 값 사용
	- 각 point cloud는 같은 모양이지만 다른 위치에 있을 것임

![[케라스 딥러닝-3장-20250308-1.png|500]]

```python
num_samples_per_class = 1000 # 첫번째 클래스의 포인트 생성 => 1000개의 랜덤한 2D포인트. cov는 왼쪽 아래서 오른쪽 위로 향하는 타원형의 point cloud를 형성

negative_samples = np.random.multivariate_normal(

    mean=[0, 3],

    cov=[[1, 0.5],[0.5, 1]],

    size=num_samples_per_class) # 두번째 클래스의포인트 생성 => 첫번쨰와 동일한 공분산 행렬과 다른 평균을 사용하여 다른 클래스의 포인트를 생성

positive_samples = np.random.multivariate_normal(

    mean=[3, 0],

    cov=[[1, 0.5],[0.5, 1]],

    size=num_samples_per_class)
```

- 각 클래스의 크기는 모두 (1000, 2) 크기의 배열 
- 각 클래스를 수직 연결해 (2000, 2) 크기의 단일 배열화 시킬 것임

```python
inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)
```

- (2000, 1)  크기의 0 배열과 1 배열을 합쳐 target label을 생성할 것임
- `inputs[i]`가 클래스 0에 속하면 `targets[i, 0]`은 0임

```python
targets = np.vstack((np.zeros((num_samples_per_class, 1), dtype="float32"),

                     np.ones((num_samples_per_class, 1), dtype="float32")))
```

**시각화**

```python
import matplotlib.pyplot as plt

plt.scatter(inputs[:, 0], inputs[:, 1], c=targets[:, 0])

plt.show()
```

![[케라스 딥러닝-3장-20250308-2.png|500]]

이제 각 point cloud를 구별할 수 있는 선형 분류기를 만들 것임
- 선형 분류기는 하나의 아핀 변환[^1]($prediction = W \cdot input + b$)임
- 예측과 타깃 사이의 차이를 제곱한 값을 최소화 하도록 훈련

각각 랜덤한 값과 0으로 초기화한  가중치 $W$와 모델 파라미터 $b$를 설정
```python
input_dim = 2 # 입력은 2D point

output_dim = 1 # 출력 예측은 샘플당 하나의 점수 (0에 가까우면 샘플을 클래스 0으로 예측, 1에 가까우면 클래스 1로 예측)

W = tf.Variable(initial_value=tf.random.uniform(shape=(input_dim, output_dim)))

b = tf.Variable(initial_value=tf.zeros(shape=(output_dim,)))
```

**정방향 패스(forward pass)를 위한 function**
```python
def model(inputs):

    return tf.matmul(inputs, W) + b  # mutmul은 텐서플로의 점곱 함수
```

- 이 선형 분류기는 2D입력을 다룸
$\rightarrow$ $W$는 2개의스칼라 가중치 $w1$과 $w2$로 이루어짐($W = w1 + w2$)
- $b$는 스칼라 값
- 어떤 입력 포인트 `[x, y]`가 주어지면 예측 값은 다음과 같이 됨
$\rightarrow prediction = [[w1], [w2] \cdot [x, y] + b = (w1 \times x) + (w2 \times y) + b$]

**loss function (MSE)**
```python
def square_loss(targets, predictions):

    per_sample_losses = tf.square(targets - predictions) #per_sample_losses는 targests나 predictions와 크기가 같은 텐서이며 각 샘플의 손실 값을 담고 있음

    return tf.reduce_mean(per_sample_losses) # reduce_mean()가 샘플당 손실 값을 하나의 스칼라 손실 값으로 평균
```

훈련 스텝으로 훈련 데이터를 받아 이 데이터에 대한 손실을 최소화하도록 $weight(W)$ 와 모델 파라미터 ($b$)를 업데이트
```python
learning_rate = 0.1

  

def training_step(inputs, targets): # 그레디언트 테이프 블록 안의 정방향 패스

    with tf.GradientTape() as tape:

        predictions = model(inputs)

        loss = square_loss(targets, predictions)

    grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b]) #가중치에 대한 손실의 그레디언트를 구함

    W.assign_sub(grad_loss_wrt_W * learning_rate)

    b.assign_sub(grad_loss_wrt_b * learning_rate)

    return loss
```

- 배치 훈련 사용
	- 데이터를 작은 배치로 나누어 반복하지 않고 전체 데이터를 사용하여 훈련 스텝 (그레이디언트 계산과 가중치 업데이트를 실행))
- 2000개 샘플에 대해 정방향 패스와 그레이디언트를 계산
	- 훈련 시간 늘어남
- 128개의 랜덤한 샘플을 사용하지 않고 전체 훈련 샘플로부터 정보를 취합
	- 각각의 그레이디언트 업데이트는 훈련 데이터의 손실을 감소하는 데 훨씬 더 효과적

결과적으로  훈련 스텝의 횟수가 많이 필요하지 않음
미니 배치 훈련 떄보다 일반적으로 큰 학습률을 사용 가능(위 코드의 learning_rate (learning_rate = 0.1)을 이용 )

```python
for step in range(40):

    loss = training_step(inputs, targets)

    print(f"{step}번째 스텝의 손실: {loss:.4f}")
```

![[케라스 딥러닝-3장-20250311.png|500|300]]

- 40번 에포크 후 훈련 손실이 0.0260으로 안정화됨

훈련 입력에 대한 모델의 예측 : 훈련 타깃과 매우 유사함  

```python
predictions = model(inputs)

plt.scatter(inputs[:,0], inputs[:, 1], c=predictions[:, 0]>0.5)

plt.show()
```
![[케라스 딥러닝-3장-20250311-1.png|500]]

포인트 `[x, y]`에 대한 예측값
- `prediction == [[w1], [w2]]`$\cdot$`[x, y] + b == w1 * x + w2 * y +b
- `class 0 == w1 * x + w2 * y + b < 0.5`
- `class 1 == w1  *  x + w2 * y + b > 0.5`

따라서 `class 0`과 `class 1`을 구분 짓는 것은 직선의 방정식
- `직선의 방정식 == w1  *  x + w2 * y + b = 0.5`
- `정규화 방정식 == - w1 / w2 *x + (0.5 - b) / w2`

이를 코드화 시키면 다음과 같음
```python
x = np.linspace(-1, 4, 100)

# 사실 100개의 x 축 좌표를 만들 필요 없이 시작과 종료 위치만 있어도 됨.
# 따라서 x = np.linspace(-1, 4, 100) 대신 x = [-1, 4] 를 사용해도 무방함

y = - W[0] /  W[1] * x + (0.5 - b) / W[1]

plt.plot(x, y, "-r")

plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] > 0.5)

plt.show()
```

![[케라스 딥러닝-3장-20250311-2.png|500]]

데이터에 있는 두클래스를 잘구분하는 직선(또는 고차원 공간의 경우 초평면(hyperplane))의 파라미터를 찾는 것 $\rightarrow$ 선형 분류기  

## 3.6 신경망의 구조 : 핵심 Keras API 이해
### 층 : 딥러닝의 구성 요소
- 신경망의 기본 데이터 구조는 2장에서 소개한 층(layers)
- 층은 하나 이상의 텐서를 출력하는 데이터 처리모듈
- 어떤 종류의 층은 상태가 없지만 대부분의 경우 $Weight$라는 층의 상태를 가짐
- $Weight$는 확률적 경사 하강법으로 학습되는 하나 이상의 텐서
	- 가중치와 모델 파라미터는 딥러닝을 통해 모델이 학습한 결과물임

layer마다 적절한 tensor format과 data processing이 다르다
- **밀집 연결 층(densely connected layer) == 완전 연결 층(fully connected layer) == 밀집 층(dense layer)**
	- (sample, features)크기의 rank-2 tensor의 벡터 데이터
- **순환 층(recurrent layer)[^2] or 1D 합성곱 층(convolution layer)(Conv1D)**
	- (sample, timesteps, features) 크기의 rank-3 tensor의 시퀸스 데이터
	#LSTM #layer
- **2D 합성곱 층(Conv2D)**
	- rank-4 tensor의 이미지 데이터
#### 케라스의 layer 클래스
- 간단한 API는 모든 것이 중심에 모인 하나의 추상화를 가져야 함
	- 케라스에서 Layer 클래스가 대표적인 예
	- 케라스에서는 Layer 또는 Layer와 밀접하게 상호 작용하는 것이 전부

Layer는 상태(Weight)와 연산(forward pass)을 캡슐화한 객체임
[[객체 지향 프로그래밍]]
- Weight(가중치)는 일반적으로 `build()` 메서드에서 정의 
- 연산은 `call()`에서 정의
- 두가지 모두 `__init__()`메서드에서 만들 수도 있지만 일반적으로는 위와 같이 정의함 

2장에서 2개의 W와 b를가지고 `output = activation(dot(input, W) + b)`계산을 수행하는 NaiveDense클래스를 정의했음
[[케라스 딥러닝 - 2장#단순한 Dense 클라스]]

다음 코드는 이와 동일한 케라스 층 코드임
```python
from tensorflow import keras

  

class SimpleDense(keras.layers.Layer): # 모든 케라스 층은 Layer 클래스를 상속해야함

  def __init__(self, units, activation=None):

    super().__init()

    self.units = units

    self.activation = activation

  

def build(self, input_shape): # build() 메서드에서 가중치를 생성

  input_dim = input_shape[-1] # 입력 텐서의 rank 정의

  self.W = self.add_weight(shape=(input_dim, self.units), initializer="random_normal") # add_weight()는가중치를간편하게 만들 수 있는 메서드임 self.W = tf.Variable(uniform(w_shape))와 같이 독립적으로 변수를 생성하고 층의 속성으로 할당 가능

  self.b = self.add_weight(shape=(self.units,), initializer="zeros")

  

def call(self, inputs): # call() 메서드에서 정방향 패스 계산을 정의

  y = tf.matmul(inputs, self.W) + self.b

  if self.activation is not None:

    y = self.activation(y)

  return y
```

<<<<<<< HEAD
```python

```

=======
 
>>>>>>> ec5bd2d0d0912237468e8360f632d3bd51cae5bb





[^1]: [[케라스 딥러닝 - 2장#텐서연산의 기하학적 해석]]

[^2]: LSTM 에서 주로 사용된다
