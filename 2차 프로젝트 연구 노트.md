# OMP 충돌 발생 시

정석은 OMP 충돌을 해결하는 것
- 환경 재구축 등

아래의 방법은 OMP 충돌을 무시 + 멀티프로세싱 비활성화 하는 것임

```bash
>>> set KMP_DUPLICATE_LIB_OK=TRUE
>>> set OMP_NUM_THREADS=1
>>> set MKL_NUM_THREADS=1
>>> yolo train model=yolov8n.pt data=data.yaml epochs=50 imgsz=640 batch=16 workers=0
```

# 환경 구축법
마지막 세번째 명령문은 requirments.txt가 있을 때 사용
```bash
>>> conda create -n yolov8_new python=3.9(해당하는 파이썬 버전 입력)
>>> conda activate yolov8_new
>>> pip install -r requirements.txt
```


# 모델 테스트 예시
```bash
yolo predict model=C:/Users/USER/Desktop/yolov8-main/yolov8-main/gkrltlfgdmsrj.v1i.yolov8.copy1/runs/detect/train/weights/best.pt source="C:/Users/USER/Desktop/yolov8-main/yolov8-main/gkrltlfgdmsrj.v1i.yolov8.copy1/test_video_0.mp4" imgsz=640 save=True conf=0.5 show=True
```

## 첫 번째 모델
```bash
(yolov8_env) C:\Users\USER\Desktop\yolov8-main\yolov8-main\gkrltlfgdmsrj.v1i.yolov8.copy

>>> yolo train model=sajeon_haksup.pt data=data.yaml epochs=50 imgsz=640 batch=16 pretrained=True lr0=0.001
```

##  두 번째 모델
```bash
>>> yolo train model=sajeon_haksup.pt data=data.yaml epochs=50 imgsz=640 batch=16 pretrained=True lr0=0.001 freeze=10
```

## 세 번째 파인튜닝
- 소요 시간 : 30m 21s
- 정확도 현저히 떨어짐
- 시도 방안 : 기존 data 와 merge 를 통해 fine tuning
- 학습 결과 : 기존 대비 성능 30% 증가
```bash
(yolov8_env) C:\Users\USER\Desktop\yolov8-main\yolov8-main\gkrltlfgdmsrj.v1i.yolov8.copy1>

>>> yolo train model=sajeon_haksup2.pt data=data.yaml epochs=100 imgsz=640 batch=16 pretrained=True lr0=0.001 freeze=10
```

## 네 번째 하이퍼 파라미터 튜닝
- 소요 시간 : 28m
- 정확도 범위 : 70 ~ 90%
- 정확도 자체의 정밀도는 증가했으나 학교 출석에 사용될 정도로 정밀하지는 않음
- 따라서 하이퍼 파라미터 튜닝 진행 : 
	- lr0=0.001 $\rightarrow$ 0.01 (학습률을 높여 혹시 모를 경사 하강시 발생할 최소 오차 방지)
	- cor_lr=False $\rightarrow$ cor_lr=True (코사인 학습률 스켈쥴러 활성화 통해 학습률을 부드럽게 감소)
- 학습 결과
	- sitae class의 분류 정밀도는 떨어졌지만 나머지 클래스의 분류 정확도가 눈에 띄게 증가됨
```bash
yolo train model=sajeon_haksup2.pt data=data.yaml epochs=100 imgsz=640 batch=16 pretrained=True lr0=0.01 freeze=10 cos_lr=True
```
## 다섯 번쨰 하이퍼 파라미터 튜닝
- 소요 시간 : 
- 기존에는 데이터 증강 기본값을 사용함(mosic=1.0, hsv_h=0.015 emd)
- 데이터 증강 수치 조정
	- close_mosic=5 $\rightarrow$ 모자이크를 마지막 epochs=10 에서 멈추던걸 epochs=5까지 더 오래 유지
	- flipud=0.5 $\rightarrow$ 상하 반적 추가
	- degrees=10.0 회전 증강(10도) 추가
	- translate=0.2, scals=0.9 $\rightarrow$ 이동 및 스케일 증강 강화
- 학습 결과 : 
	- 성능 지표 자체는 좋아졌으나, sitae class의 분류에 난항을 겪고 있음
	- 아마 클래스 자체의 데이터 수의 언벨런스함이 그 이유인 듯 하다
	- 방향은 맞는듯!

```bash
yolo train model=sajeon_haksup2.pt data=data.yaml epochs=100 imgsz=640 batch=16 pretrained=True lr0=0.01 freeze=10 cos_lr=True close_mosaic=5 flipud=0.5 degrees=10.0 translate=0.2 scale=0.9
```

## 여섯 번쨰 하이퍼 파라미터 튜닝
