# 텍스트 마이닝 = 머신러닝 + 딥러닝?

- **정의**: 텍스트 마이닝은 대량의 텍스트 데이터에서 유의미한 패턴이나 정보를 추출하는 기술.
    - **기본적으로**: 머신러닝(예: Naive Bayes, SVM)만 포함하던 시절이 있었음.
    - **요즘 트렌드**: 딥러닝(예: RNN, 트랜스포머)까지 결합해서 더 정교해졌음.  
        → 딥러닝 덕분에 문맥 이해 능력이 확 올라감.
- **데이터 전처리 필수**:  
    텍스트는 원래 지저분하니까 전처리 없으면 모델이 제대로 못 돌음.
    - **주요 과정**:
        1. **토큰화(Tokenization)**: 문장을 단어/토큰 단위로 쪼개기.
        2. **정규화(Normalization)**: 대소문자 통일, 철자 교정 등.
        3. **불용어 제거(Stopword Removal)**: "그리고", "는" 같은 의미 없는 단어 빼기.
        4. **어간 추출/표제어 추출(Stemming/Lemmatization)**: 단어의 기본 형태로 바꾸기 (예: "running" → "run").
    - 전처리 잘해야 모델 성능이 쑥 올라감. [[텍스트 데이터 전처리]]로 더 깊이 파고들 수 있음.

---

## 자연어 처리 모델: RNN → 트랜스포머로 진화

- **RNN (Recurrent Neural Network)**:
    - **특징**: 시퀀스 데이터를 순차적으로 처리. 과거 정보 기억하는 구조.
    - **장점**: 짧은 문맥 처리에 유리했음.
    - **단점**:
        1. **기울기 소실 문제(Vanishing Gradient)**: 긴 문맥에서 앞부분 정보 잊어버림.
        2. **병렬 처리 불가**: 순차 계산이라 속도 느림.
    - 그래서 LSTM, GRU 같은 변형이 나왔지만 한계는 여전했음. [[RNN 한계]] 참고.
- **트랜스포머 (Transformer)**:
    - **등장 배경**: 2017년 "Attention is All You Need" 논문에서 제안됨.
    - **핵심**: RNN 버리고 **어텐션 메커니즘**만으로 시퀀스 처리.
    - **왜 대세?**: 긴 문맥도 잘 잡고, GPU로 병렬 연산 가능해서 속도 빠름.
    - 자연어 처리의 게임체인저로 자리잡음. [[트랜스포머]]로 더 알아볼 수 있음.

---

## 트랜스포머 깊이 파헤치기

- **기본 동작**:
    - 문장 간 관계 분석에 강함. 예: "그가 책을 읽었다"와 "책은 재미있었다" 연결 이해 가능.
    - **한계**: 한 문장 안에서의 단어 간 미세한 뉘앙스는 약할 때 있음 (초기 모델 기준).
- ### 셀프 어텐션 (Self-Attention)
    
    - **정의**: 입력 시퀀스 내 단어들이 서로 얼마나 연관 있는지 스스로 계산.
        - **예시**: "고양이가 쥐를 잡았다"에서 "고양이"와 "잡았다" 관계성을 강조.
    - **구성**:
        1. **Query, Key, Value 벡터**: 단어를 세 가지 형태로 변환해서 상호작용 계산.
        2. **어텐션 스코어**: 단어 간 중요도 점수화.
        3. **소프트맥스(Softmax)**: 스코어를 확률로 바꿔 가중치 부여.
    - **장점**:
        - 문맥 따라 단어 의미 동적으로 파악.
        - RNN처럼 순차 처리 안 해도 됨 → 병렬 연산으로 효율성 쑥!
    - 이게 트랜스포머가 RNN을 압도한 핵심 원리임. [[셀프 어텐션]] 더 공부 추천.
- **추가 메커니즘**:
    - **멀티헤드 어텐션(Multi-Head Attention)**: 여러 관점에서 어텐션 계산 → 더 풍부한 이해.
    - **포지셔널 인코딩(Positional Encoding)**: 단어 순서 정보 추가 (RNN 없이도 순서 파악 가능).

--- 