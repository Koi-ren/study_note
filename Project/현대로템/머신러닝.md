---
tags:
  - 머신러닝
  - R
  - 데이터
  - 분석
  - 알고리즘
  - 트리
  - 모델링
  - 전처리
  - 통계학
---

## 1/15/2025

### Day21 – 01: R 코드 및 해석

#### R 코드

R

Collapse자동 줄바꿈복사

```R
>>>raw_data <- read.csv("binary.csv", encoding = "cp949") 
>>>head(raw_data) nrow(raw_data) 
>>>plot(raw_data) m2 <- glm(admit ~ gre, data=raw_data, family = >>>binomial(link="probit")) 
>>>summary(m2) 
>>>probit_admit <- glm(admit ~ gre + gpa + rank, data = raw_data, family = >>>binomial(link = "probit")) 
>>>probit_admit summary(probit_admit) coef(probit_admit) exp(coef(probit_admit))
```

#### 해석

- **모델 설정**:```
```R
>>>probit_admit <- glm(admit ~ gre + gpa + rank, data = raw_data, family = binomial(link = "probit"))
```    
- **출력**
        `glm(formula = admit ~ gre + gpa + rank, family = binomial(link = "probit"), data = raw_data)`
        
    - **계수(Coefficients)**:
    - 
        `(Intercept) gre gpa rank -2.091504 0.001398 0.464360 -0.331712`
        
    - **자유도(Degrees of Freedom)**:
        - Total: 399 (Null)
        - Residual: 396
    - **편차(Deviance)**:
        - Null Deviance: 500
        - Residual Deviance: 459.5
    - **AIC**: 467.5
- **상세 요약(Summary)**:
    
    
    `Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) -2.0915037 0.6718360 -3.113 0.00185 ** gre 0.0013982 0.0006487 2.156 0.03112 * gpa 0.4643598 0.1950263 2.381 0.01727 * rank -0.3317117 0.0745524 -4.449 8.61e-06 *** --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Null deviance: 499.98 on 399 degrees of freedom Residual deviance: 459.48 on 396 degrees of freedom AIC: 467.48`
    
- **계수 추출**
- 
    `(Intercept) gre gpa rank -2.091503749 0.001398222 0.464359799 -0.331711677`
    
- **지수 변환(exp)**:
    
    `(Intercept) gre gpa rank 0.1235013 1.0013992 1.5909953 0.7176942`
    

#### 해석 근거

- **Odds 식**: $Odds=eβ0+β1x1+β2x2+⋯+βqxqOdds = e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_q x_q}Odds=eβ0​+β1​x1​+β2​x2​+⋯+βq​xq​$
- **Log Odds 식**: $log⁡(Odds)=β0+β1x1+β2x2+⋯+βqxq\log(Odds) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_q x_qlog(Odds)=β0​+β1​x1​+β2​x2​+⋯+βq​xq​$

#### 주요 해석

- **gre**: 0.0013982 비율로 logit에 영향 → 긍정적 영향(합격 확률 증가).
- **rank**: -0.3317117 비율로 logit에 영향 → 부정적 영향(합격 확률 감소).
- **exp(계수)**:
    - 변수 1단위 증가 시 성공 확률에 미치는 배수(Odds 비율).
    - 예: rank가 1 증가 시 성공 확률 0.717배(약 0.7배)로 감소.

---

### Day21 – 02: R 코드 및 해석

#### R 코드
```R

>>>raw_data2 <- iris set.seed(100) 
>>>train_index <- sample(1:nrow(raw_data2), size=100, replace=F) 
>>>train_data <- raw_data2[train_index,] 
>>>str(train_data) test_data <- raw_data2[-train_index,]
>>>levels(train_data$Species) 
>>>train_data$Species <- relevel(train_data$Species, "virginica") 
>>>library(nnet) 
>>>mlogit <- multinom(Species ~., data=train_data) summary(mlogit)
```

#### 해석

- **모델 설정**:
    `mlogit <- multinom(Species ~., data=train_data)`
    
- **출력**:
    - **반복 과정**:
   
        `initial value 109.861229 iter 10 value 6.209177 ... iter 100 value 0.013915 final value 0.013915 stopped after 100 iterations`
        
    - **요약(Summary)**:
        
        `Call: multinom(formula = Species ~ ., data = train_data) Coefficients: (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width setosa 26.42213 76.82178 95.39304 -149.50448 -87.35809 versicolor 195.40425 33.50208 67.38181 -83.35251 -113.03839 Std. Errors: (Intercept) Sepal.Length Sepal.Width Petal.Length Petal.Width setosa 341.151 1376.075 1256.1716 414.043 346.496 versicolor 4838.506 1573.755 790.5269 2560.151 1746.561 Residual Deviance: 0.02782966 AIC: 20.02783`
        

#### 주요 설명

- **데이터 준비**:
    - iris 데이터 사용, 100개 샘플 무작위 추출(train_data), 나머지 test_data.
    - relevel: 기준 범주를 "virginica"로 설정(기본값 "setosa" 변경).
- **모델**: 다항 로지스틱 회귀(multinom)로 Species 예측.

---

### 머신러닝: 정형 데이터

- **데이터 구조**:
    - 2차원 배열 준비.
    - 비정형 데이터도 정형화 필요.
- **알고리즘 분류**:
    1. **지도 학습**:
        - 변량의 결과 영향 학습.
        - 예: 어떤 변량이 결과에 어떻게 작용하는지 조사.
    2. **비지도 학습**:
        - 변량/열 간 유사성 조사.
        - 목표 없이 패턴 탐색.
    3. **강화 학습**:
    4. **딥러닝**:
- **그래프와 트리**:
    - **Graph**: 노드 + 엣지.
    - **Tree**: 사이클 없는 의사결정 나무(non-cycle).
- **EDA(탐색적 데이터 분석)**:
    - 데이터 전처리 과정.
- **오류 유형**:
    - **1종 오류(Type I Error)**:
        - 귀무가설 참인데 기각 → 정밀도(Precision).
        - 예: 비즈니스에서 효과 없는 전략을 효과 있다고 판단.
    - **2종 오류(Type II Error)**:
        - 귀무가설 거짓인데 채택 → 재현율(Recall).
        - 예: 의료에서 효과 있는 치료법을 효과 없다고 판단, 코로나 음성 오류(실제 양성).
- **전처리**:
    - NA 결측치 및 이상치(outlier) 처리 필수.

---

## 1/20/2025

### 트리와 앙상블 기법

- **Tree Depth**:
    - 4단계로도 주요 변수 표현 가능(분류 규칙).
- **Xgboost**:
    - 머신러닝에서 선호.
- **앙상블 기법**:
    1. **Voting**:
        - **Hard Voting**:
            - 다수결 원칙, 정밀 반영 어려움(0/1 기준).
        - **Soft Voting**:
            - 확률 계산, 느리지만 신뢰도 높음.
    2. **Bagging**:
    3. **Boosting**:
- **다중공선성(VIF)**:
    - 확인 필요.

### 최종 목표

- 생성형 AI 평가 방법.

### 선형 회귀 사용 이유

- **간단 설명**:
    - 데이터가 선형 관계를 가질 것으로 예상 → 선형 회귀 사용.
- **상세 설명**:
    - 독립/종속 변수 간 선형 관계 예상.
    - 해석 용이, 계산 효율적, 과적합 방지.

### 차원 축소

- **선형성**:
    - 두 벡터 합 변환 = 개별 변환 합.
    - 스칼라 곱 후 변환 = 변환 후 스칼라 곱.
    - 예측 가능성 보장.

### 머신러닝 vs 딥러닝

- **머신러닝**:
    - 사람이 특성 정보(예: PCA) 취급.
- **딥러닝**:
    - 기계가 특성 정보 자동 처리.