# YOLOv8 모델 파라미터 튜닝 및 파인튜닝 과정 정리

## 1. **첫 번째 모델 학습**
### 명령어
```bash
yolo train model=yolov8n.pt data=data.yaml epochs=50 imgsz=640 batch=16 pretrained=True lr0=0.001
```

### 주요 파라미터
- **모델**: `yolov8n.pt` (YOLOv8의 Nano 버전, 경량 모델)
- **에포크**: 50
- **이미지 크기**: 640x640
- **배치 크기**: 16
- **사전 학습**: `pretrained=True` (사전 학습된 가중치 사용)
- **초기 학습률**: `lr0=0.001`

### 전략적 의도
- YOLOv8 Nano 모델을 사용하여 초기 학습을 시작.
- 낮은 학습률(`lr0=0.001`)로 안정적인 학습을 유도.
- 사전 학습된 가중치를 활용하여 학습 초기 성능을 높임.

### 학습 결과
- 초기 모델 학습으로 기본 성능을 확보.
- 추가적인 튜닝 필요성 확인.

---

## 2. **두 번째 모델 학습**
### 명령어
```bash
yolo train model=sajeon_haksup.pt data=data.yaml epochs=50 imgsz=640 batch=16 pretrained=True lr0=0.001 freeze=10
```

### 주요 변경점
- **모델**: `sajeon_haksup.pt` (첫 번째 모델 기반으로 학습된 가중치 사용)
- **고정 레이어 추가**: `freeze=10` (백본의 처음 10개 레이어 고정)

### 전략적 의도
- **고정 레이어 활용**: `freeze=10`으로 백본 레이어를 고정하여 사전 학습된 가중치를 유지하고, 나머지 레이어만 미세 조정(fine-tuning)하여 과적합 방지.
- 학습률(`lr0=0.001`)은 첫 번째 모델과 동일하게 유지하여 안정적인 학습 지속.

### 학습 결과
- 첫 번째 모델 대비 성능 개선.
- 고정 레이어로 인해 학습 초기 안정성 확보.

---

## 3. **세 번째 파인튜닝**
### 명령어
```bash
yolo train model=sajeon_haksup2.pt data=data.yaml epochs=100 imgsz=640 batch=16 pretrained=True lr0=0.001 freeze=10
```

### 주요 변경점
- **모델**: `sajeon_haksup2.pt` (두 번째 모델 기반)
- **에포크 증가**: 50 → 100
- **데이터 병합**: 기존 데이터와 새로운 데이터를 병합하여 파인튜닝 진행.

### 전략적 의도
- **데이터 병합**: 기존 데이터와 새로운 데이터를 병합하여 데이터셋의 다양성을 높이고, 모델의 일반화 성능 개선.
- **에포크 증가**: 더 긴 학습으로 모델이 데이터셋에 더 잘 적응하도록 유도.
- **고정 레이어 유지**: `freeze=10`으로 학습 안정성 유지.

### 학습 결과
- **소요 시간**: 30분 21초
- **정확도**: 초기에는 정확도가 현저히 떨어졌으나, 데이터 병합 후 성능이 기존 대비 30% 증가.
- **문제점**: 여전히 클래스 분류 성능 개선 필요.

---

## 4. **네 번째 하이퍼파라미터 튜닝**
### 명령어
```bash
yolo train model=sajeon_haksup2.pt data=data.yaml epochs=100 imgsz=640 batch=16 pretrained=True lr0=0.01 freeze=10 cos_lr=True
```

### 주요 변경점
- **학습률 증가**: `lr0=0.001` → `lr0=0.01`
- **코사인 학습률 스케줄링 추가**: `cos_lr=False` → `cos_lr=True`

### 전략적 의도
- **학습률 증가**: `lr0=0.01`로 학습률을 높여 경사 하강 시 발생할 수 있는 최소 오차를 방지하고, 학습 속도를 가속화.
- **코사인 학습률 스케줄링**: `cos_lr=True`로 학습률을 부드럽게 감소시켜 학습 후반부에 더 세밀한 최적화 유도.

### 학습 결과
- **소요 시간**: 28분
- **정확도 범위**: 70~90%
- **결과 분석**:
  - `sitae` 클래스의 분류 정밀도는 떨어졌으나, 나머지 클래스의 분류 정확도가 눈에 띄게 증가.
  - 정확도 자체의 정밀도는 증가했으나, 학교 출석에 사용될 정도로 정밀하지 않음.
- **문제점**: 특정 클래스(`sitae`)의 분류 성능 저하.

---

## 5. **다섯 번째 하이퍼파라미터 튜닝**
### 명령어
```bash
yolo train model=sajeon_haksup2.pt data=data.yaml epochs=100 imgsz=640 batch=16 pretrained=True lr0=0.01 freeze=10 cos_lr=True close_mosaic=5 flipud=0.5 degrees=10.0 translate=0.2 scale=0.9
```

### 주요 변경점
- **데이터 증강 조정**:
  - `close_mosaic=5`: 모자이크 증강을 마지막 5 에포크 동안 비활성화.
  - `flipud=0.5`: 상하 반전 증강 추가.
  - `degrees=10.0`: 10도 회전 증강 추가.
  - `translate=0.2`, `scale=0.9`: 이동 및 스케일 증강 강화.

### 전략적 의도
- **데이터 증강 강화**: 다양한 데이터 증강 기법을 추가하여 데이터셋의 다양성을 높이고, 모델의 일반화 성능 개선.
- **모자이크 증강 조정**: `close_mosaic=5`로 모자이크 증강을 더 오래 유지하여 초기 학습에서 더 다양한 패턴 학습 유도.

### 학습 결과
- **성능 지표**: 전반적인 성능 지표(mAP 등)는 개선됨.
- **문제점**:
  - `sitae` 클래스의 분류에 여전히 난항.
  - 클래스 불균형(Class Imbalance)이 원인으로 추정.
- **방향성**: 데이터 증강을 통한 일반화 성능 강화는 적절한 방향으로 보임.

---

## 6. **여섯 번째 하이퍼파라미터 튜닝**
### 명령어
```bash
yolo train model=sajeon_haksup2.pt data=data.yaml epochs=100 imgsz=640 batch=16 pretrained=True lr0=0.01 freeze=10 cos_lr=True close_mosaic=5 flipud=0.5 degrees=10.0 translate=0.2 scale=0.9 cls=3.1 box=0.15
```

### 주요 변경점
- **손실 가중치 조정**:
  - `cls=3.1`: 클래스 분류 손실 가중치 증가 (기본값 0.5 → 3.1).
  - `box=0.15`: 바운딩 박스 손실 가중치 감소 (기본값 7.5 → 0.15).

### 전략적 의도
- **예측 손실 낮추기**: `cls=3.1`로 클래스 분류 손실에 더 큰 가중치를 두어 `val/cls_loss` 감소 유도.
- **일반화 성능 강화**: 데이터 증강(`flipud`, `degrees`, `translate`, `scale`)과 `freeze=10` 설정 유지.
- **학습 안정성**: `cos_lr=True`와 `close_mosaic=5`로 학습 후반부 세밀한 최적화 유도.

### 예상 효과
- `cls=3.1`로 인해 `train/cls_loss`와 `val/cls_loss` 감소 가속화.
- `box=0.15`로 인해 바운딩 박스 손실 감소 속도 둔화 가능성 있으나, 이미 낮은 수준이므로 큰 영향 없음.
- Precision, Recall, mAP 지표에서 클래스 분류 성능 개선으로 전반적인 성능 향상 가능.

### 학습 결과
- **P-R 곡선 확인**: Precision-Recall 곡선을 통해 성능 분석 필요.
- 클래스 분류 성능 개선 여부 확인 필요.

---

## 7. **일곱 번째 하이퍼파라미터 튜닝 (최종 모델)**
### 명령어
```bash
yolo train model=sajeon_haksup2.pt data=data.yaml epochs=100 imgsz=640 batch=16 pretrained=True lr0=0.001 freeze=6 cos_lr=True close_mosaic=5 flipud=0.5 degrees=10.0 translate=0.2 scale=0.9 cls=3.1 box=0.15
```

### 주요 변경점
- **학습률 감소**: `lr0=0.01` → `lr0=0.001`
- **고정 레이어 감소**: `freeze=10` → `freeze=6`

### 전략적 의도
- **학습 안정성 강화**: `lr0=0.001`로 학습률을 낮춰 손실 함수 변동성 감소 및 안정적인 학습 유도.
- **모델 적응력 강화**: `freeze=6`으로 고정 레이어를 줄여 더 많은 레이어가 학습에 참여, 데이터셋에 더 잘 적응.
- **클래스 분류 성능 지속 개선**: `cls=3.1`, `box=0.15` 유지.

### 예상 효과
- **손실 안정화**: `train/cls_loss`와 `val/cls_loss` 감소가 더 안정적으로 이루어질 가능성.
- **성능 향상**: `freeze=6`으로 더 많은 레이어가 학습에 참여, mAP@50 및 mAP@50:95 지표 향상 가능.
- **오버피팅 방지**: 낮은 학습률과 데이터 증강 조합으로 일반화 성능 개선.

### 학습 결과
- 최종 모델 완성.
- 클래스 분류 성능 개선 및 안정적인 학습 결과 도출.

---

## 종합 분석 및 개선 방향
### 1. **전반적인 튜닝 과정 요약**
- **초기 학습 (1~2단계)**: YOLOv8 Nano 모델로 시작, 사전 학습 가중치 활용 및 고정 레이어 설정으로 안정적인 학습 진행.
- **파인튜닝 (3단계)**: 데이터 병합으로 데이터셋 다양성 확보, 성능 30% 증가.
- **하이퍼파라미터 튜닝 (4~7단계)**:
  - 학습률 조정(`lr0`), 코사인 학습률 스케줄링(`cos_lr`), 데이터 증강(`flipud`, `degrees`, `translate`, `scale`) 추가.
  - 손실 가중치 조정(`cls`, `box`)으로 클래스 분류 성능 집중 개선.
  - 고정 레이어 조정(`freeze`)으로 모델 적응력 강화.

### 2. **주요 문제점 및 해결**
- **클래스 불균형**: `sitae` 클래스의 분류 성능 저하가 지속적으로 관찰됨. 클래스 불균형이 주요 원인으로 추정.
- **해결 방안**:
  - 클래스별 데이터 분포 확인 및 부족한 클래스 데이터 추가.
  - 클래스 가중치(Class Weight) 조정.

### 3. **추가 개선 방향**
- **IoU 임계값 조정**: mAP@50:95 성능 향상을 위해 `iou` 파라미터 조정.
- **모델 아키텍처 변경**: YOLOv8의 다른 크기 모델(small, medium, large) 실험.
- **검증 데이터셋 확장**: 검증 데이터의 다양성 확보로 일반화 성능 추가 개선.

---

## 최종 모델 성능
- **mAP@50**: 0.98 (높은 성능)
- **mAP@50:95**: 0.70 (준수한 성능)
- **문제점**: `val/cls_loss`가 여전히 높게 유지, 클래스 불균형 문제 해결 필요.
- **결론**: 학교 출석 시스템에 활용 가능한 수준으로 성능 개선됨. 추가적인 클래스 불균형 해결 및 데이터 증강으로 더 높은 정밀도 달성 가능.

---

[[현대로템 2차 프로젝트 연구 노트(자체 작성)]]
[[현대로템 2차 프로젝트 기획서 2안]]