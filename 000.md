연속형 환경 시뮬레이션에서 Soft Actor-Critic (SAC)의 훈련 시간이 오래 걸린다는 주제와 관련된 논문을 찾기 위해 최근 자료를 기반으로 탐색한 결과, 아래 논문들이 관련성이 있을 것으로 보입니다. SAC의 훈련 시간 문제를 직접적으로 다루는 논문은 드물지만, 훈련 효율성, 샘플 효율성, 또는 계산 복잡도와 관련된 논문에서 간접적으로 이를 추론할 수 있습니다.

1. **"Soft Actor-Critic Algorithms and Applications" (2018)**  
   - 이 논문은 SAC의 초기 제안과 개선 사항을 다룹니다. SAC가 연속형 환경에서 샘플 효율성을 높이고 안정성을 개선했다고 주장하지만, 복잡한 환경에서 훈련 시간이 길어질 수 있다는 점을 간접적으로 시사합니다. 예를 들어, Minitaur 로봇 실험에서 2시간, Sawyer 로봇 실험에서 20시간이 소요되었다고 언급하며, 실세계 환경에서의 훈련 시간이 상당히 길어질 수 있음을 보여줍니다.

2. **"Corrected Soft Actor-Critic for Continuous Control" (2024)**  
   - 이 논문은 SAC의 샘플링 방법을 개선하여 수렴 속도와 누적 보상을 향상시키는 방법을 제안합니다. MuJoCo 환경에서 실험을 통해 SAC의 기본적인 훈련 과정이 느릴 수 있다는 점을 암시하며, 특히 tanh 변환으로 인한 왜곡이 훈련 효율성에 영향을 미친다고 지적합니다. 이는 SAC의 계산 복잡도가 훈련 시간을 늘리는 요인임을 간접적으로 나타냅니다.

3. **"Averaged Soft Actor-Critic for Deep Reinforcement Learning"**  
   - 이 논문은 SAC의 과대평가 문제를 해결하기 위해 Averaged-SAC를 제안합니다. 실험에서 K 값을 증가시키면 성능은 향상되지만 훈련 시간이 더 길어진다고 명시하며, SAC의 기본 구조가 훈련 시간에 민감하다는 점을 보여줍니다. 이는 연속형 환경에서 계산 비용이 크다는 점을 시사합니다.

4. **"Bayesian Soft Actor-Critic: A Directed Acyclic Strategy Graph Based Deep Reinforcement Learning" (2024)**  
   - 이 논문은 SAC를 기반으로 한 Bayesian SAC (BSAC)를 제안하며, OpenAI Gym 환경에서 훈련 효율성을 개선했다고 보고합니다. SAC의 기본 훈련 과정이 상대적으로 느리다는 점을 개선하려는 시도를 다루고 있어, SAC의 훈련 시간이 길다는 전제를 간접적으로 뒷받침합니다.

### 요약
위 논문들은 SAC의 훈련 시간이 길어질 수 있는 원인을 샘플 효율성, 계산 복잡도, 과대평가 문제 등으로 설명합니다. 특히 실세계 로봇 실험(예: Minitaur, Sawyer)에서 수십 시간의 훈련 시간이 소요된 사례는 연속형 환경에서 SAC의 훈련이 오래 걸릴 수 있다는 점을 잘 보여줍니다. 다만, "오래 걸린다"는 것을 정량적으로 증명하는 논문은 부족하므로, 이를 명확히 입증하려면 특정 환경에서의 비교 실험 데이터가 더 필요합니다. 추가적인 논문 탐색이나 실험 설계가 필요하다면 말씀해주세요!