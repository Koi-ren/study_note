# 논문 사례
## 논문명: Multi-objective Optimization Based Deep Reinforcement Learning for Autonomous Driving Policy

- page 6
### **원문**:
In the experiment, the average return value is output after every 5000 training iterations and is used to evaluate the algorithm. During the training phase, if a vehicle collides or drifts out of its lane, all vehicles will be reset at random position throughout the map. As show in the Figure 3, the SAC algorithm of ours using multi-constrained reward function is significantly better than the baseline method, which can reach higher reward return values faster than other methods, ==ours can obtain higher reward returns at 20,000 steps and obtain the highest return values at 70,000 steps, with relatively flat and stable curves and the final trend tends to increase. ==In contrast, the curve of the benchmark DDPG method is very volatile, and there are four curves with the lowest return values. The average return value of DQN is low and the curve is unstable and fluctuates widely, the vehicle rushes out of the lane during the turn (Figure 4) and collides with the middle barrier, no effective turning policy is learned. As shown in Figure 5, the TD3 algorithm continuously kept the vehicle stationary during the training process and could not complete normal driving, the training result shows that it cannot learn a valid driving policy.

### **번역문**:
이 실험에서는 **5000번의 학습 반복마다 평균 반환 값**을 출력하여 알고리즘의 성능을 평가했습니다. 학습 과정에서 차량이 충돌하거나 차선을 벗어나면 모든 차량이 **맵 내에서 랜덤 위치로 초기화**됩니다.

실험 결과를 보면, **SAC 알고리즘**은 여러 제약을 반영한 보상 함수를 사용하여 기존 방법보다 뛰어난 성능을 발휘했습니다. ==SAC 알고리즘은 **20,000 스텝에서 높은 반환 값을 획득**하고, **70,000 스텝에서 최고 반환 값을 기록**하며 안정적인 학습 곡선을 유지했습니다. 전체적인 추세는 **점차 증가**하는 형태를 띱니다.==

반면, **기준 DDPG 방법**의 곡선은 **매우 불안정**하며, 최소 반환 값을 기록하는 곡선이 네 개나 존재합니다. 또한, **DQN 알고리즘**은 평균 반환 값이 낮고 곡선이 크게 흔들리며 **차량이 회전 구간에서 차선을 벗어나 중앙 장애물과 충돌**하여 정상적인 주행 정책을 학습하지 못했습니다.

**TD3 알고리즘**의 경우, 학습 과정에서 차량이 계속 **정지 상태를 유지**하며 **정상적인 주행을 완료하지 못했으며, 유효한 주행 정책을 학습할 수 없었음**을 보여줍니다.

결론적으로, SAC 알고리즘이 **기존 방법들보다 더 빠르게 높은 보상을 얻고, 안정적인 주행 정책을 학습**하는 데 성공했으며, 다른 방법들은 주행 안정성이 부족하거나 정상적인 학습이 이루어지지 않았습니다.

## 논문명: Optimal scheduling of electric vehicle aggregators based on sac reinforcement learning

- page 5

### **원문**:
Taking the aggregate participation in dispatching of charging and exchange power station in city a under the typical daily scenario as an example, the average daily load scale of electric vehicles is about 12.5 MW, and the scale of sample electric vehicles is 8925. A total of 40 sets of chargers are set in the replacement station, with a maximum charging power of 75 kW and 80 sets of standby batteries. The owners hope to fully charge the charging pile before leaving. The opening hours of auxiliary services are 00:30-07:00 and 12:30-16:00, and the market price of auxiliary services is 0.2 yuan / (KWH). The peak hours are 09:00-12:00, 17:00-22:00, and the electricity price is 1.0824yuan/kwh; The valley hours are 12:00-17:00, 22:00-23:00, and the electricity price is 0.4164yuan/kwh; The normal section is 23:00 08:00, and the electricity price is 0.6644 yuan / kWh. In the calculation process, SVM adopts RBF kernel function, sets the penalty parameter to 1 and the kernel function width parameter to 1 / 6. Sac model takes 32 data as a group of training batches. In the process of neural network construction, 3 hidden layers and 64 neurons are set, and the learning rate is 10-4. After calculation, the dispatching results of electric vehicle replacement power station are as follows: The optimal scheduling scheme of adjustable electric vehicle load is solved based on sac deep reinforcement learning algorithm. ==After 16h and 1500 training times, the model training has the convergence effect as shown in Figure 2.== In order to verify the actual scheduling effect of the model, the calculation effect comparison of sac reinforcement learning, AC reinforcement learning and Pu reinforcement learning algorithms is shown in Table 1.

### **번역**:
전형적인 일일 시나리오에서 도시 A의 **충전 및 교환형 전력 스테이션의 집합적 참여**를 고려한 실험을 진행했습니다. 평균 **일일 전기차 부하 규모는 약 12.5MW**, 샘플로 사용된 **전기차 수는 총 8925대**입니다. 교환형 전력 스테이션에는 **40개의 충전기가 설치**되어 있으며, **최대 충전 출력은 75kW**, 그리고 **80개의 예비 배터리가 준비**되어 있습니다. 차량 소유자는 출발 전에 충전소에서 **완전히 충전되기를 희망**합니다.

**보조 서비스 운영 시간 및 전기 가격 정보**:

- **보조 서비스 운영 시간**: 00:30~07:00, 12:30~16:00
    
- **보조 서비스 시장 가격**: 0.2 위안 / kWh
    
- **피크 시간대**: 09:00~12:00, 17:00~22:00 (전기 가격: 1.0824 위안/kWh)
    
- **계곡 시간대**: 12:00~17:00, 22:00~23:00 (전기 가격: 0.4164 위안/kWh)
    
- **일반 시간대**: 23:00~08:00 (전기 가격: 0.6644 위안/kWh)
    

**모델 학습 과정**:

- SVM은 RBF 커널 함수를 사용하며, **페널티 파라미터를 1로 설정**하고, **커널 함수 너비를 1/6**으로 설정.
    
- SAC 모델은 **32개의 데이터 그룹을 하나의 훈련 배치로 구성**.
    
- 신경망 구축 과정에서 **3개의 은닉층(hidden layers) 및 64개의 뉴런(neurons)을 설정**.
    
- 학습률(learning rate)은 **10⁻⁴**로 설정.
    

**SAC 심층 강화 학습 알고리즘 기반 최적 스케줄링 결과**:

- ==**총 16시간 동안 1500번의 학습**을 진행한 후 모델이 수렴됨.==
    
- **모델의 실제 스케줄링 효과를 검증**하기 위해 SAC 강화 학습, AC 강화 학습, PG 강화 학습 알고리즘의 성능을 비교.
## 논문명: Automated Parking Trajectory Generation Using Deep Reinforcement Learning

- page 3

### **원문**: 
To validate the effectiveness of the SAC algorithm for autonomous parking, we conducted simulation experiments using Gazebo 11.0 integrated with ROS Noetic to simulate realistic vehicle dynamics and sensor data, approximating real world conditions. The test vehicle controlled via continuous steering angle (-30° to 30°) and throttle/brake (-1 to 1), aligning 
with SAC’s continuous action space. Control commands and sensor data were exchanged through ROS 1 topics for real-time interaction. Three scenarios were designed: (1) parallel parking; (2) perpendicular parking; and (3) complex mixed parking. Each scenario varied initial vehicle positions and orientations to increase complexity. The SAC algorithm, implemented in PyTorch, used a policy network and value network, each with three fully connected layers. The policy network outputted action means and standard deviations for steering and throttle, while the value network estimated Q-values. The state space included vehicle position (x, y), orientation (θ), speed (v), distance to the parking space, obstacle distances, and parking space geometry. The action space was a two-dimensional vector for steering and throttle/brake. TableⅠshows the training parameters of SAC.

### **번역**:
자율 주차에서 SAC 알고리즘의 효과를 검증하기 위해, **Gazebo 11.0과 ROS Noetic을 통합한 시뮬레이션 실험**을 수행했습니다. 이를 통해 실제 차량 동역학 및 센서 데이터를 구현하여 **현실 세계 조건을 모방**하였습니다.

테스트 차량은 **연속적인 조향각(-30° ~ 30°) 및 가속/브레이크 입력(-1 ~ 1)**을 사용하여 SAC의 연속적인 행동 공간과 일치하도록 제어되었습니다. **ROS 1 토픽을 통해 실시간으로 센서 데이터 및 제어 명령을 교환**하며 주차 작업을 수행했습니다.

**세 가지 주차 시나리오를 설계**하여 테스트를 진행했습니다:

1. **평행 주차**
    
2. **직각 주차**
    
3. **복합(혼합) 주차**
    

각 시나리오는 차량의 초기 위치와 방향을 다양하게 설정하여 복잡성을 증가시켰습니다.

SAC 알고리즘은 **PyTorch를 활용하여 구현**되었으며, **정책 네트워크**와 **가치 네트워크**를 포함한 세 개의 완전 연결층으로 구성되었습니다.

- **정책 네트워크**: 조향 및 가속 입력에 대한 평균과 표준 편차를 출력.
    
- **가치 네트워크**: 상태 공간에 대한 Q-값을 추정.
    

**상태 공간(state space) 구성 요소**:

- 차량 위치 (**x, y**)
    
- 차량 방향 (**θ**)
    
- 차량 속도 (**v**)
    
- 주차 공간까지의 거리
    
- 장애물 거리 정보
    
- 주차 공간 형상
    

**행동 공간(action space)**:

- 조향각 및 가속/브레이크 입력을 포함한 **2차원 벡터**.
![[강화학습 학습 시간 관련-20250522.png|500]]

## 논문명: Continuous World Coverage Path Planning for Fixed-Wing UAVs using Deep Reinforcement Learning

- page 

### **원문**:
### **번역**:
