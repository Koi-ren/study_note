---
tags:
  - 딥러닝
  - 신경망
  - 케라스
  - 현대로템
---
[[딥러닝 강의 요약]]

# 1번 문제
## 딥러닝
자극이라고 하는 것은 크기가 있다.
수의 형태로 여러 개가 들어가면 각각의 자극에 준하는 정도(가중치)가 있고 합산된 정보가 들어 갈 것이다.

이 값을 변환해주는 어떤 함수를 걸쳤다고 볼 수 있는 것 --> 인공 신경망의 핵심

뉴런은 입력과 출력을 갖고 있다 like 컴퓨터 함수

위 이론이 인공 신경망 이론의 모태가 됨

이를 tensorflow, keras에서는 layer라고 칭함 --> 활성함수 또는 판별함수(relu, softmax, sigmoid 등)

사람이 왜 고등 동물인가? --> 뉴런이 많아서
뉴런을 어떻게 작동 시키느냐에 따라 판단이 다르더라
컴퓨터도 똑같다.

특성정보를 뽑아내는 것(feature, sequence 등) --> 이를 rank-i tensor 등으로 설명 가능

데이터마다 tensor의 스타일이 다르다. --> transformore 기반으로 하는 llm 등

활성화 함수라기 보다는 퍼셉트론(MDP)이라고 불러야함

# 2번 문제
## 머신러닝과 딥러닝 차이

**feature** 추출을 기계가 **스스로** 학습하는 것

# 3번 문제

기울기 소실(Vanishing Gradient)문제는 사실 까먹는 것을 의미(층이 깊어질 수록)

**매우 중요함!!**

고등 딥러닝 모델로 갈 수록 층은 깊어지기에...
res(잔차-residual)을 다시 집어 넣어서

## 4번 문제

역전파 알고리즘

## 5번 문제

**손실 함수(Loss Function)의 역할** ? 
**b) 모델의 예측 값과 실제 값의 차이를 측정하는 역할**

# 6번 문제

Gradient Boosting은 xgboost 등 머신러닝의 분류 모델에 사용되는 방법론임

# 7번 문제

시계열에서는 순차데이터에 적합한 RNN(Recurrent Neural Networks)이 사용된다.

RNN은 이전의 정보들을 메모리에 저장하여 현재의 예측에 반영하는데, 이는 시간적 순서가 중요한 데이터를 분석하는데 특히 적합

RNN의 대표적인 한계는 **장기 종속성 문제**로, 긴 시퀀스에서의 학습이 어려울 수 있음

# 8번 문제

너무 깊은 네트워크는 과적합을 일으킨다 --> 한가지 사례만을 계속해서 분석하기 때문(Vanishing Gradient 문제에 빠질 수 있따.)

정규화는 과적합을 방지하기 위한 방법이다. 

# 9번 문제

ReLU는 음수 값을 0으로 변환하기 때문에 ‘Dying ReLU’ 문제가 발생할 수 있음

![[Pasted image 20250304165914.png]]